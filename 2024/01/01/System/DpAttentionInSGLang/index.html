<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>SGLang中的DP Attention原理与DeepSeekMoE适配 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="SGLang中的DP Attention原理与DeepSeekMoE适配概述SGLang实现了专门针对DeepSeek系列模型的DP Attention（数据并行注意力）机制，这是一个创新的并行化方案，用于解决Multi-Head Latent Attention (MLA)在传统张量并行下的KV缓存重复问题。本文详细介绍DP Attention的工作原理以及它如何与DeepSeekMoE架构完美">
<meta property="og:type" content="article">
<meta property="og:title" content="SGLang中的DP Attention原理与DeepSeekMoE适配">
<meta property="og:url" content="http://example.com/2024/01/01/System/DpAttentionInSGLang/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="SGLang中的DP Attention原理与DeepSeekMoE适配概述SGLang实现了专门针对DeepSeek系列模型的DP Attention（数据并行注意力）机制，这是一个创新的并行化方案，用于解决Multi-Head Latent Attention (MLA)在传统张量并行下的KV缓存重复问题。本文详细介绍DP Attention的工作原理以及它如何与DeepSeekMoE架构完美">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-12-31T16:00:00.000Z">
<meta property="article:modified_time" content="2025-07-11T04:23:28.920Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="System">
<meta property="article:tag" content="SGLang">
<meta property="article:tag" content="DeepSeek">
<meta property="article:tag" content="MoE">
<meta property="article:tag" content="Attention">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

  
  <!-- Mermaid -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      securityLevel: 'loose',
      themeVariables: {
        primaryColor: '#0f4c75',
        primaryTextColor: '#fff',
        primaryBorderColor: '#0f4c75',
        lineColor: '#0f4c75',
        secondaryColor: '#006ba6',
        tertiaryColor: '#fff'
      }
    });
  </script>
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-System/DpAttentionInSGLang" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/01/01/System/DpAttentionInSGLang/" class="article-date">
  <time class="dt-published" datetime="2023-12-31T16:00:00.000Z" itemprop="datePublished">2024-01-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/System/">System</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      SGLang中的DP Attention原理与DeepSeekMoE适配
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="SGLang中的DP-Attention原理与DeepSeekMoE适配"><a href="#SGLang中的DP-Attention原理与DeepSeekMoE适配" class="headerlink" title="SGLang中的DP Attention原理与DeepSeekMoE适配"></a>SGLang中的DP Attention原理与DeepSeekMoE适配</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>SGLang实现了专门针对DeepSeek系列模型的DP Attention（数据并行注意力）机制，这是一个创新的并行化方案，用于解决Multi-Head Latent Attention (MLA)在传统张量并行下的KV缓存重复问题。本文详细介绍DP Attention的工作原理以及它如何与DeepSeekMoE架构完美结合。</p>
<h2 id="背景：MLA的挑战"><a href="#背景：MLA的挑战" class="headerlink" title="背景：MLA的挑战"></a>背景：MLA的挑战</h2><h3 id="传统张量并行的问题"><a href="#传统张量并行的问题" class="headerlink" title="传统张量并行的问题"></a>传统张量并行的问题</h3><p>在传统的Multi-Head Attention (MHA)中，张量并行(TP)通过将KV缓存按头数维度切分到不同设备上来实现并行化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传统MHA的张量并行</span></span><br><span class="line"><span class="comment"># 假设有8个头，TP=8，每个设备处理1个头</span></span><br><span class="line">device_0: heads[<span class="number">0</span>]</span><br><span class="line">device_1: heads[<span class="number">1</span>]</span><br><span class="line">...</span><br><span class="line">device_7: heads[<span class="number">7</span>]</span><br></pre></td></tr></table></figure>

<h3 id="MLA的特殊性"><a href="#MLA的特殊性" class="headerlink" title="MLA的特殊性"></a>MLA的特殊性</h3><p>DeepSeek的Multi-Head Latent Attention具有以下特点：</p>
<ol>
<li><strong>单一KV头</strong>: MLA的<code>head_num</code>为1，无法按头数切分</li>
<li><strong>低秩压缩</strong>: 使用压缩的潜在向量<code>c_kv</code>存储KV信息</li>
<li><strong>共享键值</strong>: 所有查询头共享相同的键值表示</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MLA的结构</span></span><br><span class="line">c_kv = W_down_kv @ hidden_states  <span class="comment"># 压缩的KV表示</span></span><br><span class="line">k_heads = W_up_k @ c_kv           <span class="comment"># 所有头共享相同的键</span></span><br><span class="line">v_heads = W_up_v @ c_kv           <span class="comment"># 所有头共享相同的值</span></span><br></pre></td></tr></table></figure>

<p>这导致在传统TP下，每个设备都必须保存完整的KV缓存，造成存储冗余。</p>
<h2 id="DP-Attention原理"><a href="#DP-Attention原理" class="headerlink" title="DP Attention原理"></a>DP Attention原理</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>DP Attention采用<strong>数据并行</strong>而非<strong>张量并行</strong>的方式处理MLA：</p>
<ul>
<li><strong>按请求分割</strong>: 将不同的请求分配给不同的设备</li>
<li><strong>阶段分离</strong>: 不同设备可以处理不同的计算阶段（prefill&#x2F;decode）</li>
<li><strong>通信协调</strong>: 在MLA和MoE之间进行必要的通信同步</li>
</ul>
<h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DP Attention的工作流程</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DPAttention</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states, batch_info</span>):</span><br><span class="line">        <span class="comment"># 1. 每个设备处理分配给它的请求</span></span><br><span class="line">        local_hidden_states = hidden_states[device_batch_slice]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 本地MLA计算</span></span><br><span class="line">        local_output = <span class="variable language_">self</span>.mla_forward(local_hidden_states)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. AllGather收集所有设备的输出</span></span><br><span class="line">        global_output = all_gather(local_output)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. MoE计算（需要全局信息）</span></span><br><span class="line">        moe_output = <span class="variable language_">self</span>.moe_forward(global_output)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 5. 切片提取本设备负责的结果</span></span><br><span class="line">        final_output = moe_output[device_batch_slice]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure>

<h3 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 内存使用对比</span></span><br><span class="line"><span class="comment"># 传统TP (8设备)</span></span><br><span class="line">memory_per_device = &#123;</span><br><span class="line">    <span class="string">&#x27;model_params&#x27;</span>: total_params / tp_size,</span><br><span class="line">    <span class="string">&#x27;kv_cache&#x27;</span>: full_kv_cache,  <span class="comment"># 每个设备都需要完整KV缓存</span></span><br><span class="line">    <span class="string">&#x27;activations&#x27;</span>: batch_size * hidden_size</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DP Attention (8设备)</span></span><br><span class="line">memory_per_device = &#123;</span><br><span class="line">    <span class="string">&#x27;model_params&#x27;</span>: total_params,  <span class="comment"># 每个设备有完整参数</span></span><br><span class="line">    <span class="string">&#x27;kv_cache&#x27;</span>: kv_cache / dp_size,  <span class="comment"># KV缓存按请求分割</span></span><br><span class="line">    <span class="string">&#x27;activations&#x27;</span>: (batch_size / dp_size) * hidden_size</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="与DeepSeekMoE的集成"><a href="#与DeepSeekMoE的集成" class="headerlink" title="与DeepSeekMoE的集成"></a>与DeepSeekMoE的集成</h2><h3 id="DeepSeekMoE架构回顾"><a href="#DeepSeekMoE架构回顾" class="headerlink" title="DeepSeekMoE架构回顾"></a>DeepSeekMoE架构回顾</h3><p>DeepSeekMoE采用以下设计：</p>
<ol>
<li><strong>共享专家</strong>: 所有token都会访问的专家</li>
<li><strong>路由专家</strong>: 通过路由机制选择的专家</li>
<li><strong>细粒度分割</strong>: 专家参数进行更细致的分割</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DeepSeekMoE结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DeepSeekMoE</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.shared_experts = nn.ModuleList(shared_experts)</span><br><span class="line">        <span class="variable language_">self</span>.routed_experts = nn.ModuleList(routed_experts)</span><br><span class="line">        <span class="variable language_">self</span>.router = Router()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 共享专家处理</span></span><br><span class="line">        shared_output = <span class="built_in">sum</span>(expert(x) <span class="keyword">for</span> expert <span class="keyword">in</span> <span class="variable language_">self</span>.shared_experts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 路由专家处理</span></span><br><span class="line">        router_weights = <span class="variable language_">self</span>.router(x)</span><br><span class="line">        routed_output = <span class="variable language_">self</span>.route_to_experts(x, router_weights)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x + shared_output + routed_output</span><br></pre></td></tr></table></figure>

<h3 id="DP-Attention与MoE的协作"><a href="#DP-Attention与MoE的协作" class="headerlink" title="DP Attention与MoE的协作"></a>DP Attention与MoE的协作</h3><h4 id="1-通信模式"><a href="#1-通信模式" class="headerlink" title="1. 通信模式"></a>1. 通信模式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DP Attention + MoE的通信模式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dp_attention_moe_forward</span>(<span class="params">hidden_states</span>):</span><br><span class="line">    <span class="comment"># 阶段1: 本地MLA计算</span></span><br><span class="line">    local_attn_output = local_mla_attention(hidden_states)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 阶段2: AllGather - 收集所有设备的注意力输出</span></span><br><span class="line">    global_attn_output = all_gather(local_attn_output)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 阶段3: MoE计算 - 需要全局信息进行专家路由</span></span><br><span class="line">    moe_output = deepseek_moe_forward(global_attn_output)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 阶段4: Slice - 提取本设备负责的部分</span></span><br><span class="line">    local_final_output = moe_output[local_slice]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> local_final_output</span><br></pre></td></tr></table></figure>

<h4 id="2-专家路由适配"><a href="#2-专家路由适配" class="headerlink" title="2. 专家路由适配"></a>2. 专家路由适配</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 专家路由在DP环境下的适配</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DPAdaptedRouter</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_experts, dp_size</span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_experts = num_experts</span><br><span class="line">        <span class="variable language_">self</span>.dp_size = dp_size</span><br><span class="line">        <span class="variable language_">self</span>.device_expert_map = <span class="variable language_">self</span>._create_expert_mapping()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="comment"># 路由计算需要考虑设备分布</span></span><br><span class="line">        routing_weights = <span class="variable language_">self</span>.compute_routing_weights(tokens)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 专家负载均衡</span></span><br><span class="line">        balanced_weights = <span class="variable language_">self</span>.balance_across_devices(routing_weights)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> balanced_weights</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_expert_mapping</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 将专家均匀分布到不同设备</span></span><br><span class="line">        experts_per_device = <span class="variable language_">self</span>.num_experts // <span class="variable language_">self</span>.dp_size</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            device_id: <span class="built_in">list</span>(<span class="built_in">range</span>(</span><br><span class="line">                device_id * experts_per_device,</span><br><span class="line">                (device_id + <span class="number">1</span>) * experts_per_device</span><br><span class="line">            ))</span><br><span class="line">            <span class="keyword">for</span> device_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.dp_size)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-负载均衡机制"><a href="#3-负载均衡机制" class="headerlink" title="3. 负载均衡机制"></a>3. 负载均衡机制</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 专家负载均衡</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ExpertLoadBalancer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dp_size, num_experts</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dp_size = dp_size</span><br><span class="line">        <span class="variable language_">self</span>.num_experts = num_experts</span><br><span class="line">        <span class="variable language_">self</span>.expert_usage_stats = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">balance_load</span>(<span class="params">self, routing_decisions</span>):</span><br><span class="line">        <span class="comment"># 统计每个设备的专家使用情况</span></span><br><span class="line">        device_loads = <span class="variable language_">self</span>.calculate_device_loads(routing_decisions)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 动态调整路由以平衡负载</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.is_imbalanced(device_loads):</span><br><span class="line">            routing_decisions = <span class="variable language_">self</span>.rebalance_routing(routing_decisions)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> routing_decisions</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate_device_loads</span>(<span class="params">self, routing_decisions</span>):</span><br><span class="line">        loads = [<span class="number">0</span>] * <span class="variable language_">self</span>.dp_size</span><br><span class="line">        <span class="keyword">for</span> token_routing <span class="keyword">in</span> routing_decisions:</span><br><span class="line">            <span class="keyword">for</span> expert_id <span class="keyword">in</span> token_routing:</span><br><span class="line">                device_id = <span class="variable language_">self</span>.get_expert_device(expert_id)</span><br><span class="line">                loads[device_id] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> loads</span><br></pre></td></tr></table></figure>

<h2 id="性能优化策略"><a href="#性能优化策略" class="headerlink" title="性能优化策略"></a>性能优化策略</h2><h3 id="1-通信优化"><a href="#1-通信优化" class="headerlink" title="1. 通信优化"></a>1. 通信优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 异步通信优化</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AsyncDPAttention</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.comm_stream = torch.cuda.Stream()</span><br><span class="line">        <span class="variable language_">self</span>.comp_stream = torch.cuda.Stream()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">        <span class="keyword">with</span> torch.cuda.stream(<span class="variable language_">self</span>.comp_stream):</span><br><span class="line">            <span class="comment"># 本地MLA计算</span></span><br><span class="line">            local_output = <span class="variable language_">self</span>.mla_forward(hidden_states)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.cuda.stream(<span class="variable language_">self</span>.comm_stream):</span><br><span class="line">            <span class="comment"># 异步AllGather</span></span><br><span class="line">            gathered_output = <span class="variable language_">self</span>.async_all_gather(local_output)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 同步点</span></span><br><span class="line">        torch.cuda.synchronize()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> gathered_output</span><br></pre></td></tr></table></figure>

<h3 id="2-内存优化"><a href="#2-内存优化" class="headerlink" title="2. 内存优化"></a>2. 内存优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 内存池管理</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MemoryPoolManager</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dp_size</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dp_size = dp_size</span><br><span class="line">        <span class="variable language_">self</span>.kv_cache_pools = &#123;&#125;</span><br><span class="line">        <span class="variable language_">self</span>.activation_pools = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">allocate_kv_cache</span>(<span class="params">self, batch_size, seq_len</span>):</span><br><span class="line">        <span class="comment"># 按设备分配KV缓存</span></span><br><span class="line">        cache_size = batch_size // <span class="variable language_">self</span>.dp_size</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.get_or_create_cache(cache_size, seq_len)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_or_create_cache</span>(<span class="params">self, cache_size, seq_len</span>):</span><br><span class="line">        key = (cache_size, seq_len)</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.kv_cache_pools:</span><br><span class="line">            <span class="variable language_">self</span>.kv_cache_pools[key] = torch.empty(</span><br><span class="line">                cache_size, seq_len, <span class="variable language_">self</span>.hidden_size,</span><br><span class="line">                dtype=torch.float16, device=<span class="string">&#x27;cuda&#x27;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.kv_cache_pools[key]</span><br></pre></td></tr></table></figure>

<h3 id="3-调度优化"><a href="#3-调度优化" class="headerlink" title="3. 调度优化"></a>3. 调度优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 批次调度优化</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DPBatchScheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dp_size</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dp_size = dp_size</span><br><span class="line">        <span class="variable language_">self</span>.device_queues = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dp_size)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">schedule_requests</span>(<span class="params">self, requests</span>):</span><br><span class="line">        <span class="comment"># 根据请求特征分配到不同设备</span></span><br><span class="line">        <span class="keyword">for</span> request <span class="keyword">in</span> requests:</span><br><span class="line">            device_id = <span class="variable language_">self</span>.select_device(request)</span><br><span class="line">            <span class="variable language_">self</span>.device_queues[device_id].append(request)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_device</span>(<span class="params">self, request</span>):</span><br><span class="line">        <span class="comment"># 负载均衡策略</span></span><br><span class="line">        loads = [<span class="built_in">len</span>(queue) <span class="keyword">for</span> queue <span class="keyword">in</span> <span class="variable language_">self</span>.device_queues]</span><br><span class="line">        <span class="keyword">return</span> loads.index(<span class="built_in">min</span>(loads))</span><br></pre></td></tr></table></figure>

<h2 id="实际应用案例"><a href="#实际应用案例" class="headerlink" title="实际应用案例"></a>实际应用案例</h2><h3 id="1-启动配置"><a href="#1-启动配置" class="headerlink" title="1. 启动配置"></a>1. 启动配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动带有DP Attention的SGLang服务</span></span><br><span class="line">python -m sglang.launch_server \</span><br><span class="line">    --model deepseek-ai/DeepSeek-V3 \</span><br><span class="line">    --tp 8 \</span><br><span class="line">    --dp 8 \</span><br><span class="line">    --enable-dp-attention \</span><br><span class="line">    --trust-remote-code \</span><br><span class="line">    --mem-fraction-static 0.9</span><br></pre></td></tr></table></figure>

<h3 id="2-性能对比"><a href="#2-性能对比" class="headerlink" title="2. 性能对比"></a>2. 性能对比</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 性能测试结果</span></span><br><span class="line">performance_comparison = &#123;</span><br><span class="line">    <span class="string">&#x27;traditional_tp&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;memory_per_device&#x27;</span>: <span class="string">&#x27;40GB&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;throughput&#x27;</span>: <span class="string">&#x27;1000 tokens/s&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;latency&#x27;</span>: <span class="string">&#x27;100ms&#x27;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;dp_attention&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;memory_per_device&#x27;</span>: <span class="string">&#x27;25GB&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;throughput&#x27;</span>: <span class="string">&#x27;1900 tokens/s&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;latency&#x27;</span>: <span class="string">&#x27;85ms&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-扩展性分析"><a href="#3-扩展性分析" class="headerlink" title="3. 扩展性分析"></a>3. 扩展性分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 扩展性测试</span></span><br><span class="line">scaling_results = &#123;</span><br><span class="line">    <span class="string">&#x27;batch_size_1&#x27;</span>: &#123;<span class="string">&#x27;improvement&#x27;</span>: <span class="string">&#x27;1.2x&#x27;</span>&#125;,</span><br><span class="line">    <span class="string">&#x27;batch_size_8&#x27;</span>: &#123;<span class="string">&#x27;improvement&#x27;</span>: <span class="string">&#x27;1.6x&#x27;</span>&#125;,</span><br><span class="line">    <span class="string">&#x27;batch_size_32&#x27;</span>: &#123;<span class="string">&#x27;improvement&#x27;</span>: <span class="string">&#x27;1.9x&#x27;</span>&#125;,</span><br><span class="line">    <span class="string">&#x27;batch_size_128&#x27;</span>: &#123;<span class="string">&#x27;improvement&#x27;</span>: <span class="string">&#x27;2.1x&#x27;</span>&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="16用户请求处理流程图"><a href="#16用户请求处理流程图" class="headerlink" title="16用户请求处理流程图"></a>16用户请求处理流程图</h2><p>下图展示了SGLang中DP Attention处理16个用户请求的完整流程，包括数据并行分配、MLA层处理、专家路由和输出聚合的全过程：</p>
<pre class="mermaid">graph TD
    subgraph "输入层 - 16个用户请求"
        U1[Request 1] 
        U2[Request 2]
        U3[Request 3]
        U4[Request 4]
        U5[Request 5]
        U6[Request 6]
        U7[Request 7]
        U8[Request 8]
        U9[Request 9]
        U10[Request 10]
        U11[Request 11]
        U12[Request 12]
        U13[Request 13]
        U14[Request 14]
        U15[Request 15]
        U16[Request 16]
    end

    subgraph "数据并行分配 - 4个节点"
        subgraph "Node 0"
            N0[Requests 1-4<br/>Batch Size: 4]
        end
        subgraph "Node 1"
            N1[Requests 5-8<br/>Batch Size: 4]
        end
        subgraph "Node 2"
            N2[Requests 9-12<br/>Batch Size: 4]
        end
        subgraph "Node 3"
            N3[Requests 13-16<br/>Batch Size: 4]
        end
    end

    subgraph "MLA层 - DP Attention处理"
        subgraph "Node 0 MLA"
            MLA0[MLA Computation<br/>KV Cache: Local 4 requests<br/>c_kv compression]
        end
        subgraph "Node 1 MLA"
            MLA1[MLA Computation<br/>KV Cache: Local 4 requests<br/>c_kv compression]
        end
        subgraph "Node 2 MLA"
            MLA2[MLA Computation<br/>KV Cache: Local 4 requests<br/>c_kv compression]
        end
        subgraph "Node 3 MLA"
            MLA3[MLA Computation<br/>KV Cache: Local 4 requests<br/>c_kv compression]
        end
    end

    subgraph "AllGather - 注意力输出聚合"
        AG[AllGather Operation<br/>收集所有节点的注意力输出<br/>每个节点获得完整的16个输出]
    end

    subgraph "MoE层 - 专家路由与并行"
        subgraph "Expert Routing"
            ER[Token-level Expert Routing<br/>每个token根据门控网络<br/>路由到对应专家]
        end
        
        subgraph "Expert Parallel Processing"
            subgraph "Node 0 - Expert 0,1"
                E0[Expert 0<br/>处理路由到的tokens]
                E1[Expert 1<br/>处理路由到的tokens]
            end
            subgraph "Node 1 - Expert 2,3"
                E2[Expert 2<br/>处理路由到的tokens]
                E3[Expert 3<br/>处理路由到的tokens]
            end
            subgraph "Node 2 - Expert 4,5"
                E4[Expert 4<br/>处理路由到的tokens]
                E5[Expert 5<br/>处理路由到的tokens]
            end
            subgraph "Node 3 - Expert 6,7"
                E6[Expert 6<br/>处理路由到的tokens]
                E7[Expert 7<br/>处理路由到的tokens]
            end
        end
    end

    subgraph "AllToAll - 专家输出重分布"
        ATA[AllToAll Communication<br/>将专家输出重新分布<br/>回到原始的数据并行分割]
    end

    subgraph "输出层 - 结果聚合"
        subgraph "Node 0 Output"
            O0[Output for<br/>Requests 1-4]
        end
        subgraph "Node 1 Output"
            O1[Output for<br/>Requests 5-8]
        end
        subgraph "Node 2 Output"
            O2[Output for<br/>Requests 9-12]
        end
        subgraph "Node 3 Output"
            O3[Output for<br/>Requests 13-16]
        end
    end

    subgraph "最终输出"
        OUT[16个完整的推理结果]
    end

    %% 连接关系
    U1 --> N0
    U2 --> N0
    U3 --> N0
    U4 --> N0
    U5 --> N1
    U6 --> N1
    U7 --> N1
    U8 --> N1
    U9 --> N2
    U10 --> N2
    U11 --> N2
    U12 --> N2
    U13 --> N3
    U14 --> N3
    U15 --> N3
    U16 --> N3

    N0 --> MLA0
    N1 --> MLA1
    N2 --> MLA2
    N3 --> MLA3

    MLA0 --> AG
    MLA1 --> AG
    MLA2 --> AG
    MLA3 --> AG

    AG --> ER
    ER --> E0
    ER --> E1
    ER --> E2
    ER --> E3
    ER --> E4
    ER --> E5
    ER --> E6
    ER --> E7

    E0 --> ATA
    E1 --> ATA
    E2 --> ATA
    E3 --> ATA
    E4 --> ATA
    E5 --> ATA
    E6 --> ATA
    E7 --> ATA

    ATA --> O0
    ATA --> O1
    ATA --> O2
    ATA --> O3

    O0 --> OUT
    O1 --> OUT
    O2 --> OUT
    O3 --> OUT

    %% 样式
    classDef nodeStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef mlaStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef expertStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef commStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef outputStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px

    class N0,N1,N2,N3 nodeStyle
    class MLA0,MLA1,MLA2,MLA3 mlaStyle
    class E0,E1,E2,E3,E4,E5,E6,E7 expertStyle
    class AG,ATA commStyle
    class O0,O1,O2,O3,OUT outputStyle</pre>

<h3 id="流程关键点说明"><a href="#流程关键点说明" class="headerlink" title="流程关键点说明"></a>流程关键点说明</h3><ol>
<li><strong>数据并行分配</strong>：16个请求均匀分配到4个节点，每个节点处理4个请求</li>
<li><strong>MLA层处理</strong>：每个节点独立处理本地请求，只需维护1&#x2F;4的KV缓存</li>
<li><strong>AllGather聚合</strong>：收集所有节点的注意力输出，为MoE层提供完整上下文</li>
<li><strong>专家路由</strong>：基于门控网络将tokens路由到不同专家</li>
<li><strong>专家并行</strong>：8个专家分布在4个节点上并行处理</li>
<li><strong>AllToAll重分布</strong>：将专家输出重新分布回原始的数据并行布局</li>
<li><strong>结果输出</strong>：每个节点输出对应请求的完整结果</li>
</ol>
<blockquote>
<p>流程图源文件：<a href="../images/sglang-dpattention-flow.mermaid">sglang-dpattention-flow.mermaid</a></p>
</blockquote>
<h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><h3 id="1-通信原语使用"><a href="#1-通信原语使用" class="headerlink" title="1. 通信原语使用"></a>1. 通信原语使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGLang中使用的通信原语</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DPCommunication</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dp_size</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dp_size = dp_size</span><br><span class="line">        <span class="variable language_">self</span>.comm_group = dist.new_group(<span class="built_in">range</span>(dp_size))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">all_gather_attention_output</span>(<span class="params">self, local_output</span>):</span><br><span class="line">        <span class="comment"># 收集所有设备的注意力输出</span></span><br><span class="line">        gathered_outputs = [torch.zeros_like(local_output) </span><br><span class="line">                          <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.dp_size)]</span><br><span class="line">        dist.all_gather(gathered_outputs, local_output, </span><br><span class="line">                       group=<span class="variable language_">self</span>.comm_group)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(gathered_outputs, dim=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scatter_moe_input</span>(<span class="params">self, global_input</span>):</span><br><span class="line">        <span class="comment"># 分散MoE输入到各设备</span></span><br><span class="line">        chunk_size = global_input.size(<span class="number">0</span>) // <span class="variable language_">self</span>.dp_size</span><br><span class="line">        chunks = torch.split(global_input, chunk_size, dim=<span class="number">0</span>)</span><br><span class="line">        local_chunk = torch.empty_like(chunks[<span class="number">0</span>])</span><br><span class="line">        dist.scatter(local_chunk, <span class="built_in">list</span>(chunks) <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span> <span class="keyword">else</span> [], </span><br><span class="line">                    src=<span class="number">0</span>, group=<span class="variable language_">self</span>.comm_group)</span><br><span class="line">        <span class="keyword">return</span> local_chunk</span><br></pre></td></tr></table></figure>

<h3 id="2-错误处理与容错"><a href="#2-错误处理与容错" class="headerlink" title="2. 错误处理与容错"></a>2. 错误处理与容错</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 容错机制</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DPFaultTolerance</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dp_size</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dp_size = dp_size</span><br><span class="line">        <span class="variable language_">self</span>.health_status = [<span class="literal">True</span>] * dp_size</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">handle_device_failure</span>(<span class="params">self, failed_device_id</span>):</span><br><span class="line">        <span class="comment"># 设备故障处理</span></span><br><span class="line">        <span class="variable language_">self</span>.health_status[failed_device_id] = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 重新分配负载</span></span><br><span class="line">        healthy_devices = [i <span class="keyword">for</span> i, status <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.health_status) <span class="keyword">if</span> status]</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.redistribute_load(healthy_devices)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">redistribute_load</span>(<span class="params">self, healthy_devices</span>):</span><br><span class="line">        <span class="comment"># 负载重分配逻辑</span></span><br><span class="line">        new_dp_size = <span class="built_in">len</span>(healthy_devices)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.create_new_dp_group(healthy_devices, new_dp_size)</span><br></pre></td></tr></table></figure>

<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><h3 id="1-配置建议"><a href="#1-配置建议" class="headerlink" title="1. 配置建议"></a>1. 配置建议</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推荐配置</span></span><br><span class="line">recommended_config = &#123;</span><br><span class="line">    <span class="string">&#x27;small_batch&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;dp_size&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">&#x27;tp_size&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">&#x27;enable_overlap&#x27;</span>: <span class="literal">True</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;large_batch&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;dp_size&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&#x27;tp_size&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;enable_overlap&#x27;</span>: <span class="literal">True</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;mixed_workload&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;dp_size&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">&#x27;tp_size&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">&#x27;enable_overlap&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;adaptive_scheduling&#x27;</span>: <span class="literal">True</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-监控指标"><a href="#2-监控指标" class="headerlink" title="2. 监控指标"></a>2. 监控指标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关键监控指标</span></span><br><span class="line">monitoring_metrics = &#123;</span><br><span class="line">    <span class="string">&#x27;communication_overhead&#x27;</span>: <span class="string">&#x27;AllGather和Scatter的耗时&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;load_balance&#x27;</span>: <span class="string">&#x27;各设备间的负载均衡度&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;memory_utilization&#x27;</span>: <span class="string">&#x27;KV缓存的内存使用率&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;expert_utilization&#x27;</span>: <span class="string">&#x27;专家的使用频率分布&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;throughput&#x27;</span>: <span class="string">&#x27;整体吞吐量&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;latency&#x27;</span>: <span class="string">&#x27;端到端延迟&#x27;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-调优策略"><a href="#3-调优策略" class="headerlink" title="3. 调优策略"></a>3. 调优策略</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 性能调优</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DPTuningStrategy</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.tuning_params = &#123;</span><br><span class="line">            <span class="string">&#x27;comm_overlap_ratio&#x27;</span>: <span class="number">0.8</span>,</span><br><span class="line">            <span class="string">&#x27;batch_size_threshold&#x27;</span>: <span class="number">64</span>,</span><br><span class="line">            <span class="string">&#x27;expert_capacity_factor&#x27;</span>: <span class="number">1.2</span>,</span><br><span class="line">            <span class="string">&#x27;load_balance_threshold&#x27;</span>: <span class="number">0.1</span></span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">auto_tune</span>(<span class="params">self, workload_characteristics</span>):</span><br><span class="line">        <span class="comment"># 根据工作负载特征自动调优</span></span><br><span class="line">        <span class="keyword">if</span> workload_characteristics[<span class="string">&#x27;avg_batch_size&#x27;</span>] &gt; <span class="number">64</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.large_batch_config()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.small_batch_config()</span><br></pre></td></tr></table></figure>

<h2 id="未来发展方向"><a href="#未来发展方向" class="headerlink" title="未来发展方向"></a>未来发展方向</h2><h3 id="1-算法优化"><a href="#1-算法优化" class="headerlink" title="1. 算法优化"></a>1. 算法优化</h3><ul>
<li><strong>更智能的负载均衡</strong>: 基于历史负载模式的预测性调度</li>
<li><strong>动态专家分配</strong>: 根据访问模式动态调整专家分布</li>
<li><strong>通信压缩</strong>: 利用量化和压缩技术减少通信开销</li>
</ul>
<h3 id="2-硬件适配"><a href="#2-硬件适配" class="headerlink" title="2. 硬件适配"></a>2. 硬件适配</h3><ul>
<li><strong>新架构支持</strong>: 适配更新的GPU架构和interconnect</li>
<li><strong>异构计算</strong>: 支持CPU-GPU混合计算</li>
<li><strong>分布式存储</strong>: 与分布式存储系统的深度集成</li>
</ul>
<h3 id="3-系统集成"><a href="#3-系统集成" class="headerlink" title="3. 系统集成"></a>3. 系统集成</h3><ul>
<li><strong>容器化部署</strong>: 更好的容器化和编排支持</li>
<li><strong>云原生</strong>: 与Kubernetes等云原生技术的集成</li>
<li><strong>边缘计算</strong>: 适配边缘计算场景的轻量化版本</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SGLang的DP Attention机制为DeepSeek系列模型提供了高效的并行化解决方案，通过数据并行而非传统的张量并行，成功解决了MLA架构下的KV缓存重复问题。与DeepSeekMoE的集成进一步提升了系统的整体性能和扩展性。</p>
<p><strong>关键优势</strong>:</p>
<ol>
<li><strong>内存效率</strong>: 显著减少KV缓存的内存占用</li>
<li><strong>计算效率</strong>: 提高整体计算吞吐量</li>
<li><strong>扩展性</strong>: 支持更大规模的批处理</li>
<li><strong>适应性</strong>: 能够适应不同的工作负载特征</li>
</ol>
<p><strong>应用建议</strong>:</p>
<ol>
<li>大批量场景优先使用DP Attention</li>
<li>合理配置DP和TP的比例</li>
<li>关注通信开销和负载均衡</li>
<li>持续监控和优化性能指标</li>
</ol>
<p>这种创新的并行化策略为大规模语言模型的高效部署提供了新的思路，值得在实际应用中深入探索和优化。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/01/01/System/DpAttentionInSGLang/" data-id="cmdpf31a90019q0on7fud5pre" data-title="SGLang中的DP Attention原理与DeepSeekMoE适配" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepSeek/" rel="tag">DeepSeek</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MoE/" rel="tag">MoE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SGLang/" rel="tag">SGLang</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/System/" rel="tag">System</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/01/15/mermaid-test/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Mermaid Charts Test Page
        
      </div>
    </a>
  
  
    <a href="/2024/01/01/System/%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">通信原语 - 大模型推理中的分布式通信</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/System/">System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Test/">Test</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Communication/" rel="tag">Communication</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Compiler/" rel="tag">Compiler</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Graph/" rel="tag">Computational Graph</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepSeek/" rel="tag">DeepSeek</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distributed/" rel="tag">Distributed</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MoE/" rel="tag">MoE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization/" rel="tag">Optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Performance/" rel="tag">Performance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SGLang/" rel="tag">SGLang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/" rel="tag">System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/" rel="tag">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blues/" rel="tag">blues</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/career/" rel="tag">career</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/charts/" rel="tag">charts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/consensus/" rel="tag">consensus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/data-parallelism/" rel="tag">data-parallelism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/distributed-systems/" rel="tag">distributed-systems</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/experience/" rel="tag">experience</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/expert-parallelism/" rel="tag">expert-parallelism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/guitar/" rel="tag">guitar</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/harmony/" rel="tag">harmony</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/licks/" rel="tag">licks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/" rel="tag">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mermaid/" rel="tag">mermaid</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/raft/" rel="tag">raft</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rhythm/" rel="tag">rhythm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/test/" rel="tag">test</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/Communication/" style="font-size: 10px;">Communication</a> <a href="/tags/Compiler/" style="font-size: 10px;">Compiler</a> <a href="/tags/Computational-Graph/" style="font-size: 10px;">Computational Graph</a> <a href="/tags/DeepSeek/" style="font-size: 10px;">DeepSeek</a> <a href="/tags/Distributed/" style="font-size: 10px;">Distributed</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/MoE/" style="font-size: 10px;">MoE</a> <a href="/tags/Optimization/" style="font-size: 10px;">Optimization</a> <a href="/tags/Performance/" style="font-size: 10px;">Performance</a> <a href="/tags/SGLang/" style="font-size: 10px;">SGLang</a> <a href="/tags/System/" style="font-size: 15px;">System</a> <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/blues/" style="font-size: 20px;">blues</a> <a href="/tags/career/" style="font-size: 10px;">career</a> <a href="/tags/charts/" style="font-size: 10px;">charts</a> <a href="/tags/consensus/" style="font-size: 10px;">consensus</a> <a href="/tags/data-parallelism/" style="font-size: 10px;">data-parallelism</a> <a href="/tags/distributed-systems/" style="font-size: 20px;">distributed-systems</a> <a href="/tags/experience/" style="font-size: 10px;">experience</a> <a href="/tags/expert-parallelism/" style="font-size: 10px;">expert-parallelism</a> <a href="/tags/guitar/" style="font-size: 20px;">guitar</a> <a href="/tags/harmony/" style="font-size: 10px;">harmony</a> <a href="/tags/licks/" style="font-size: 10px;">licks</a> <a href="/tags/life/" style="font-size: 10px;">life</a> <a href="/tags/llm/" style="font-size: 15px;">llm</a> <a href="/tags/mermaid/" style="font-size: 10px;">mermaid</a> <a href="/tags/raft/" style="font-size: 10px;">raft</a> <a href="/tags/rhythm/" style="font-size: 10px;">rhythm</a> <a href="/tags/test/" style="font-size: 10px;">test</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/05/">May 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/07/30/System/%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/%E5%BA%8F%E5%88%97%E5%B9%B6%E8%A1%8C/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/28/PD%E5%88%86%E7%A6%BB%E6%9E%B6%E6%9E%84KVCache%E8%B7%A8%E8%8A%82%E7%82%B9%E4%BC%A0%E8%BE%93%E5%8A%9F%E8%83%BD%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BC%98%E5%8C%96KVCache%E4%BC%A0%E8%BE%93%E6%95%88%E7%8E%87/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/28/System/%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/EP/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/28/System/%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E5%9F%BA%E7%A1%80/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/28/System/DeepSeek/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






<!-- Mermaid Support -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
<script>
  mermaid.initialize({
    startOnLoad: true,
    theme: 'default',
    securityLevel: 'loose',
    themeVariables: {
      primaryColor: '#0f4c75',
      primaryTextColor: '#fff',
      primaryBorderColor: '#0f4c75',
      lineColor: '#0f4c75',
      secondaryColor: '#006ba6',
      tertiaryColor: '#fff'
    }
  });
</script>
  </div>
</body>
</html>