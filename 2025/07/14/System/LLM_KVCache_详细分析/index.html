<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="LLM Serving Framework KV Cache 管理机制详细分析目录 概述 TensorRT-LLM KV Cache 管理 vLLM KV Cache 管理   SGLang KV Cache 管理 技术对比与深度分析 性能优化策略 总结与建议  概述KV Cache是大语言模型推理中最关键的优化技术之一，它通过缓存attention机制中的Key和Value张量来避免重复计算，显">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2025/07/14/System/LLM_KVCache_%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="LLM Serving Framework KV Cache 管理机制详细分析目录 概述 TensorRT-LLM KV Cache 管理 vLLM KV Cache 管理   SGLang KV Cache 管理 技术对比与深度分析 性能优化策略 总结与建议  概述KV Cache是大语言模型推理中最关键的优化技术之一，它通过缓存attention机制中的Key和Value张量来避免重复计算，显">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-07-14T03:21:11.624Z">
<meta property="article:modified_time" content="2025-07-16T09:34:06.094Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

  
  <!-- Mermaid -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      securityLevel: 'loose',
      themeVariables: {
        primaryColor: '#0f4c75',
        primaryTextColor: '#fff',
        primaryBorderColor: '#0f4c75',
        lineColor: '#0f4c75',
        secondaryColor: '#006ba6',
        tertiaryColor: '#fff'
      }
    });
  </script>
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-System/LLM_KVCache_详细分析" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/14/System/LLM_KVCache_%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/" class="article-date">
  <time class="dt-published" datetime="2025-07-14T03:21:11.624Z" itemprop="datePublished">2025-07-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="LLM-Serving-Framework-KV-Cache-管理机制详细分析"><a href="#LLM-Serving-Framework-KV-Cache-管理机制详细分析" class="headerlink" title="LLM Serving Framework KV Cache 管理机制详细分析"></a>LLM Serving Framework KV Cache 管理机制详细分析</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li><a href="#%E6%A6%82%E8%BF%B0">概述</a></li>
<li><a href="#tensorrt-llm-kv-cache-%E7%AE%A1%E7%90%86">TensorRT-LLM KV Cache 管理</a></li>
<li><a href="#vllm-kv-cache-%E7%AE%A1%E7%90%86">vLLM KV Cache 管理</a>  </li>
<li><a href="#sglang-kv-cache-%E7%AE%A1%E7%90%86">SGLang KV Cache 管理</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90">技术对比与深度分析</a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5">性能优化策略</a></li>
<li><a href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E5%BB%BA%E8%AE%AE">总结与建议</a></li>
</ol>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>KV Cache是大语言模型推理中最关键的优化技术之一，它通过缓存attention机制中的Key和Value张量来避免重复计算，显著提升推理性能。本文深入分析TensorRT-LLM、vLLM和SGLang三个主流LLM serving框架的KV Cache管理实现。</p>
<h3 id="KV-Cache基本原理"><a href="#KV-Cache基本原理" class="headerlink" title="KV Cache基本原理"></a>KV Cache基本原理</h3><p>在Transformer架构中，每个attention层都需要计算Q、K、V三个矩阵：</p>
<ul>
<li><strong>Query (Q)</strong>: 当前token的查询向量</li>
<li><strong>Key (K)</strong>: 所有token的键向量  </li>
<li><strong>Value (V)</strong>: 所有token的值向量</li>
</ul>
<p>KV Cache的核心思想是：</p>
<ol>
<li><strong>缓存已计算的K、V</strong>: 避免重复计算历史token的K、V</li>
<li><strong>增量更新</strong>: 只计算新token的K、V并追加到缓存</li>
<li><strong>内存复用</strong>: 通过block管理和prefix sharing减少内存占用</li>
</ol>
<h2 id="TensorRT-LLM-KV-Cache-管理"><a href="#TensorRT-LLM-KV-Cache-管理" class="headerlink" title="TensorRT-LLM KV Cache 管理"></a>TensorRT-LLM KV Cache 管理</h2><h3 id="整体架构设计"><a href="#整体架构设计" class="headerlink" title="整体架构设计"></a>整体架构设计</h3><p>TensorRT-LLM采用<strong>分层block管理架构</strong>，具有以下核心组件：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 核心类层次结构</span></span><br><span class="line">KVCacheManager</span><br><span class="line">├── BlockManager</span><br><span class="line">│   └── <span class="built_in">WindowBlockManager</span> (多个，按window size分组)</span><br><span class="line">│       ├── <span class="built_in">KVCacheBlockPool</span> (内存池)</span><br><span class="line">│       ├── <span class="built_in">LRUEvictionPolicy</span> (驱逐策略)</span><br><span class="line">│       └── <span class="built_in">KVCacheTransferManager</span> (传输管理)</span><br><span class="line">├── <span class="built_in">KVCacheEventManager</span> (事件管理)</span><br><span class="line">└── <span class="built_in">AllocateKvCache</span> (分配策略)</span><br></pre></td></tr></table></figure>

<h3 id="1-Block管理机制"><a href="#1-Block管理机制" class="headerlink" title="1. Block管理机制"></a>1. Block管理机制</h3><h4 id="Block数据结构"><a href="#Block数据结构" class="headerlink" title="Block数据结构"></a>Block数据结构</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KVCacheBlock</span> &#123;</span><br><span class="line">    IdType mBlockId;                    <span class="comment">// 块ID</span></span><br><span class="line">    tk::KVCacheIndex mMemoryPoolBlockIndex; <span class="comment">// 内存池索引</span></span><br><span class="line">    <span class="type">int32_t</span> mRefCount;                  <span class="comment">// 引用计数</span></span><br><span class="line">    <span class="type">int32_t</span> mSchedulingRefCount;        <span class="comment">// 调度引用计数</span></span><br><span class="line">    BlockPtr mPrevBlock;                <span class="comment">// 前驱块指针</span></span><br><span class="line">    NextBlockMap mNextBlocks;           <span class="comment">// 后继块映射</span></span><br><span class="line">    <span class="type">bool</span> mIsFull;                       <span class="comment">// 是否已满</span></span><br><span class="line">    executor::RetentionPriority mPriority; <span class="comment">// 优先级</span></span><br><span class="line">    std::optional&lt;std::chrono::milliseconds&gt; mDurationMs; <span class="comment">// 持续时间</span></span><br><span class="line">    <span class="type">size_t</span> mHash;                       <span class="comment">// 哈希值</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h4 id="Block分配策略"><a href="#Block分配策略" class="headerlink" title="Block分配策略"></a>Block分配策略</h4><ol>
<li><strong>分池管理</strong>: 根据KV head数量分组，相同配置的layer共享池</li>
<li><strong>优先级分配</strong>: 支持设置block优先级和过期时间</li>
<li><strong>引用计数</strong>: 精确跟踪block使用情况，支持共享和释放</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Block分配核心逻辑</span></span><br><span class="line"><span class="function">BlockPtr <span class="title">WindowBlockManager::getFreeBlock</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    executor::RetentionPriority priority, </span></span></span><br><span class="line"><span class="params"><span class="function">    std::optional&lt;std::chrono::milliseconds&gt; durationMs)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 1. 从驱逐策略获取空闲block</span></span><br><span class="line">    <span class="keyword">auto</span> [block, canOffload] = mEvictionPolicy-&gt;<span class="built_in">getFreeBlock</span>(kPrimaryLevel);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2. 如果需要，执行offload到secondary memory</span></span><br><span class="line">    <span class="keyword">if</span> (!block-&gt;<span class="built_in">getUniqueTokens</span>().<span class="built_in">empty</span>() &amp;&amp; canOffload &amp;&amp; </span><br><span class="line">        mEvictionPolicy-&gt;<span class="built_in">getNumFreeBlocks</span>(kSecondaryLevel) &gt; <span class="number">0</span> &amp;&amp; mOnboardBlocks) &#123;</span><br><span class="line">        <span class="keyword">auto</span> offloadBlock = std::<span class="built_in">get</span>&lt;<span class="number">0</span>&gt;(mEvictionPolicy-&gt;<span class="built_in">getFreeBlock</span>(kSecondaryLevel));</span><br><span class="line">        mTransferManager-&gt;<span class="built_in">offload</span>(block, offloadBlock, mPools);</span><br><span class="line">        block-&gt;<span class="built_in">swapMemoryPoolBlockOffset</span>(offloadBlock);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> block;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-内存池管理"><a href="#2-内存池管理" class="headerlink" title="2. 内存池管理"></a>2. 内存池管理</h3><h4 id="分层内存设计"><a href="#分层内存设计" class="headerlink" title="分层内存设计"></a>分层内存设计</h4><ul>
<li><strong>Primary Pool</strong>: GPU快速内存，用于活跃block</li>
<li><strong>Secondary Pool</strong>: CPU较慢内存，用于offload</li>
<li><strong>Block Pool</strong>: 按layer配置分组的物理内存</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KVCacheBlockPool</span> &#123;</span><br><span class="line">    SizeType32 numLayers;           <span class="comment">// layer数量</span></span><br><span class="line">    SizeType32 kvFactor;            <span class="comment">// KV因子(通常为2，K和V)</span></span><br><span class="line">    SizeType32 numKvHeads;          <span class="comment">// KV head数量</span></span><br><span class="line">    SizeType32 tokensPerBlock;      <span class="comment">// 每个block的token数</span></span><br><span class="line">    runtime::ITensor::SharedPtr primaryPtr;   <span class="comment">// 主内存池</span></span><br><span class="line">    runtime::ITensor::SharedPtr secondaryPtr; <span class="comment">// 辅助内存池</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h4 id="内存分配流程"><a href="#内存分配流程" class="headerlink" title="内存分配流程"></a>内存分配流程</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">WindowBlockManager::allocatePools</span><span class="params">(<span class="type">bool</span> useUvm)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; pool : mPools) &#123;</span><br><span class="line">        <span class="comment">// 计算block大小</span></span><br><span class="line">        <span class="keyword">auto</span> blockSize = pool.blockSize;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 主内存池分配 - 形状: [num_blocks, num_layers, kv_factor, block_size]</span></span><br><span class="line">        nvinfer1::Dims cacheShape = ITensor::<span class="built_in">makeShape</span>(</span><br><span class="line">            &#123;mNumPrimaryBlocks, pool.numLayers, mKVFactor, blockSize&#125;);</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> (useUvm)</span><br><span class="line">            pool.primaryPtr = BufferManager::<span class="built_in">managed</span>(cacheShape, poolDtype);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            pool.primaryPtr = mBufferManager.<span class="built_in">gpuSync</span>(cacheShape, poolDtype);</span><br><span class="line">            </span><br><span class="line">        <span class="comment">// 辅助内存池分配</span></span><br><span class="line">        <span class="keyword">if</span> (mNumSecondaryBlocks &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            nvinfer1::Dims cacheShapeOffload = ITensor::<span class="built_in">makeShape</span>(</span><br><span class="line">                &#123;mNumSecondaryBlocks, pool.numLayers, mKVFactor, blockSize&#125;);</span><br><span class="line">            pool.secondaryPtr = BufferManager::<span class="built_in">pinned</span>(cacheShapeOffload, poolDtype);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-哈希与缓存复用"><a href="#3-哈希与缓存复用" class="headerlink" title="3. 哈希与缓存复用"></a>3. 哈希与缓存复用</h3><h4 id="高性能哈希算法"><a href="#高性能哈希算法" class="headerlink" title="高性能哈希算法"></a>高性能哈希算法</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">BlockKeyHasher::hash</span><span class="params">(BlockKey <span class="type">const</span>&amp; blockKey, std::<span class="type">size_t</span> parentHash)</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    <span class="type">size_t</span> seed = blockKey.uniqueTokens.<span class="built_in">size</span>() ^ parentHash * <span class="built_in">UINT64_C</span>(<span class="number">0xbf58476d1ce4e5b9</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> <span class="type">const</span>&amp; uniqueToken : blockKey.uniqueTokens) &#123;</span><br><span class="line">        <span class="comment">// 使用Wang hash算法优化token ID哈希</span></span><br><span class="line">        <span class="type">uint32_t</span> a = <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(uniqueToken.tokenId);</span><br><span class="line">        a = ((a &gt;&gt; <span class="number">16</span>) ^ a) * <span class="number">0x45d9f3b</span>;</span><br><span class="line">        a = ((a &gt;&gt; <span class="number">16</span>) ^ a) * <span class="number">0x45d9f3b</span>; </span><br><span class="line">        a = (a &gt;&gt; <span class="number">16</span>) ^ a;</span><br><span class="line">        seed ^= a + <span class="number">0x9e3779b9</span> + (seed &lt;&lt; <span class="number">6</span>) + (seed &gt;&gt; <span class="number">2</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 处理额外ID（如LoRA task ID）</span></span><br><span class="line">        <span class="keyword">if</span> (blockKey.usesExtraIds) &#123;</span><br><span class="line">            <span class="type">uint64_t</span> b = uniqueToken.tokenExtraId;</span><br><span class="line">            <span class="comment">// 64位哈希处理...</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> seed;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-驱逐策略"><a href="#4-驱逐策略" class="headerlink" title="4. 驱逐策略"></a>4. 驱逐策略</h3><h4 id="LRU驱逐算法"><a href="#LRU驱逐算法" class="headerlink" title="LRU驱逐算法"></a>LRU驱逐算法</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LRUEvictionPolicy</span> &#123;</span><br><span class="line">    <span class="comment">// 分优先级的队列结构</span></span><br><span class="line">    std::vector&lt;std::vector&lt;FreeBlocksQueue&gt;&gt; mFreeQueues; <span class="comment">// [cache_level][priority]</span></span><br><span class="line">    std::vector&lt;std::unordered_set&lt;SizeType32&gt;&gt; mReleasedBlocks;</span><br><span class="line">    std::vector&lt;SizeType32&gt; mNumFreeBlocksPerLevel;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 获取空闲block</span></span><br><span class="line">    <span class="function">std::tuple&lt;BlockPtr, <span class="type">bool</span>&gt; <span class="title">getFreeBlock</span><span class="params">(SizeType32 level)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 按优先级顺序查找空闲block</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> priority = kMinPriority; priority &lt;= kMaxPriority; ++priority) &#123;</span><br><span class="line">            <span class="keyword">auto</span>&amp; freeQueue = mFreeQueues[level][<span class="built_in">getPriorityIdx</span>(priority)];</span><br><span class="line">            <span class="keyword">if</span> (!freeQueue.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">                <span class="keyword">auto</span> block = freeQueue.<span class="built_in">front</span>();</span><br><span class="line">                freeQueue.<span class="built_in">pop_front</span>();</span><br><span class="line">                <span class="keyword">return</span> &#123;block, <span class="literal">true</span>&#125;;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="literal">nullptr</span>, <span class="literal">false</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h2 id="vLLM-KV-Cache-管理"><a href="#vLLM-KV-Cache-管理" class="headerlink" title="vLLM KV Cache 管理"></a>vLLM KV Cache 管理</h2><h3 id="整体架构设计-1"><a href="#整体架构设计-1" class="headerlink" title="整体架构设计"></a>整体架构设计</h3><p>vLLM采用<strong>模块化block分配架构</strong>，核心设计理念是简洁性和可扩展性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心类层次结构  </span></span><br><span class="line">SelfAttnBlockSpaceManager</span><br><span class="line">├── CpuGpuBlockAllocator</span><br><span class="line">│   ├── PrefixCachingBlockAllocator (GPU)</span><br><span class="line">│   └── PrefixCachingBlockAllocator (CPU)</span><br><span class="line">├── ComputedBlocksTracker (prefix缓存跟踪)</span><br><span class="line">└── LastAccessBlocksTracker (访问时间跟踪)</span><br></pre></td></tr></table></figure>

<h3 id="1-Block分配器设计"><a href="#1-Block分配器设计" class="headerlink" title="1. Block分配器设计"></a>1. Block分配器设计</h3><h4 id="设备感知分配器"><a href="#设备感知分配器" class="headerlink" title="设备感知分配器"></a>设备感知分配器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CpuGpuBlockAllocator</span>(<span class="title class_ inherited__">DeviceAwareBlockAllocator</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cpu_block_allocator: BlockAllocator, </span></span><br><span class="line"><span class="params">                 gpu_block_allocator: BlockAllocator</span>):</span><br><span class="line">        <span class="variable language_">self</span>._allocators = &#123;</span><br><span class="line">            Device.CPU: cpu_block_allocator,</span><br><span class="line">            Device.GPU: gpu_block_allocator,</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="variable language_">self</span>._swap_mapping: <span class="type">Dict</span>[<span class="built_in">int</span>, <span class="built_in">int</span>] = &#123;&#125;  <span class="comment"># CPU-GPU交换映射</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">allocate_mutable_block</span>(<span class="params">self, prev_block: <span class="type">Optional</span>[Block], </span></span><br><span class="line"><span class="params">                              device: Device, extra_hash: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>) -&gt; Block:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._allocators[device].allocate_mutable_block(prev_block, extra_hash=extra_hash)</span><br></pre></td></tr></table></figure>

<h4 id="PrefixCaching分配器"><a href="#PrefixCaching分配器" class="headerlink" title="PrefixCaching分配器"></a>PrefixCaching分配器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PrefixCachingBlockAllocator</span>(<span class="title class_ inherited__">BlockAllocator</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_blocks: <span class="built_in">int</span>, block_size: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU</span>):</span><br><span class="line">        <span class="comment"># prefix hash到block ID的映射</span></span><br><span class="line">        <span class="variable language_">self</span>._cached_blocks: <span class="type">Dict</span>[PrefixHash, BlockId] = &#123;&#125;</span><br><span class="line">        <span class="comment"># 被调度器触及的不可变block ID集合</span></span><br><span class="line">        <span class="variable language_">self</span>._touched_blocks: <span class="type">Set</span>[BlockId] = <span class="built_in">set</span>()</span><br><span class="line">        <span class="comment"># 每个物理block ID的状态跟踪</span></span><br><span class="line">        <span class="variable language_">self</span>._block_tracker: <span class="type">Dict</span>[BlockId, BlockTracker] = &#123;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-Block生命周期管理"><a href="#2-Block生命周期管理" class="headerlink" title="2. Block生命周期管理"></a>2. Block生命周期管理</h3><h4 id="Block状态跟踪"><a href="#Block状态跟踪" class="headerlink" title="Block状态跟踪"></a>Block状态跟踪</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BlockTracker</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.active: <span class="built_in">bool</span> = <span class="literal">False</span>           <span class="comment"># 是否活跃</span></span><br><span class="line">        <span class="variable language_">self</span>.last_accessed: <span class="built_in">float</span> = -<span class="number">1</span>      <span class="comment"># 最后访问时间</span></span><br><span class="line">        <span class="variable language_">self</span>.computed: <span class="built_in">bool</span> = <span class="literal">False</span>         <span class="comment"># 是否已计算</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">enable</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.active = <span class="literal">True</span></span><br><span class="line">        <span class="variable language_">self</span>.reset()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.last_accessed = _DEFAULT_LAST_ACCESSED_TIME</span><br><span class="line">        <span class="variable language_">self</span>.computed = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<h4 id="Immutable-vs-Mutable-Block"><a href="#Immutable-vs-Mutable-Block" class="headerlink" title="Immutable vs Mutable Block"></a>Immutable vs Mutable Block</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">allocate_immutable_block</span>(<span class="params">self, prev_block: <span class="type">Optional</span>[Block], </span></span><br><span class="line"><span class="params">                           token_ids: <span class="type">List</span>[<span class="built_in">int</span>], extra_hash: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>) -&gt; Block:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;分配不可变block用于prefix caching&quot;&quot;&quot;</span></span><br><span class="line">    block = <span class="variable language_">self</span>._create_block(prev_block, token_ids, <span class="variable language_">self</span>._block_size, </span><br><span class="line">                              <span class="variable language_">self</span>, computed=<span class="literal">True</span>, extra_hash=extra_hash)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查是否已缓存</span></span><br><span class="line">    <span class="keyword">if</span> block.content_hash <span class="keyword">in</span> <span class="variable language_">self</span>._cached_blocks:</span><br><span class="line">        cached_block_id = <span class="variable language_">self</span>._cached_blocks[block.content_hash]</span><br><span class="line">        <span class="variable language_">self</span>._incr_refcount_cached_block(<span class="variable language_">self</span>._blocks[cached_block_id])</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._blocks[cached_block_id]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 分配新block</span></span><br><span class="line">    <span class="variable language_">self</span>._cached_blocks[block.content_hash] = block.block_id</span><br><span class="line">    <span class="keyword">return</span> block</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">allocate_mutable_block</span>(<span class="params">self, prev_block: <span class="type">Optional</span>[Block], </span></span><br><span class="line"><span class="params">                          extra_hash: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>) -&gt; Block:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;分配可变block用于新生成的token&quot;&quot;&quot;</span></span><br><span class="line">    block_id = <span class="variable language_">self</span>._allocate_block_id()</span><br><span class="line">    block = <span class="variable language_">self</span>._create_block(prev_block, [], <span class="variable language_">self</span>._block_size, </span><br><span class="line">                              <span class="variable language_">self</span>, block_id, extra_hash=extra_hash)</span><br><span class="line">    <span class="keyword">return</span> block</span><br></pre></td></tr></table></figure>

<h3 id="3-内容哈希机制"><a href="#3-内容哈希机制" class="headerlink" title="3. 内容哈希机制"></a>3. 内容哈希机制</h3><h4 id="层次化哈希计算"><a href="#层次化哈希计算" class="headerlink" title="层次化哈希计算"></a>层次化哈希计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PrefixCachingBlock</span>(<span class="title class_ inherited__">Block</span>):</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hash_block_tokens</span>(<span class="params">cls, is_first_block: <span class="built_in">bool</span>, prev_block_hash: <span class="type">Optional</span>[<span class="built_in">int</span>],</span></span><br><span class="line"><span class="params">                         cur_block_token_ids: <span class="type">List</span>[<span class="built_in">int</span>], extra_hash: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算block的内容哈希&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> is_first_block:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">hash</span>((<span class="built_in">tuple</span>(cur_block_token_ids), extra_hash))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">hash</span>((prev_block_hash, <span class="built_in">tuple</span>(cur_block_token_ids), extra_hash))</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">content_hash</span>(<span class="params">self</span>) -&gt; <span class="type">Optional</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>._content_hash <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="variable language_">self</span>.token_ids:</span><br><span class="line">            <span class="variable language_">self</span>._content_hash = <span class="variable language_">self</span>.hash_block_tokens(</span><br><span class="line">                <span class="variable language_">self</span>._prev_block <span class="keyword">is</span> <span class="literal">None</span>,</span><br><span class="line">                <span class="variable language_">self</span>._prev_block.content_hash <span class="keyword">if</span> <span class="variable language_">self</span>._prev_block <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                <span class="variable language_">self</span>.token_ids,</span><br><span class="line">                <span class="variable language_">self</span>.extra_hash</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._content_hash</span><br></pre></td></tr></table></figure>

<h3 id="4-调度集成"><a href="#4-调度集成" class="headerlink" title="4. 调度集成"></a>4. 调度集成</h3><h4 id="Memory-aware调度"><a href="#Memory-aware调度" class="headerlink" title="Memory-aware调度"></a>Memory-aware调度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttnBlockSpaceManager</span>(<span class="title class_ inherited__">BlockSpaceManager</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">can_allocate</span>(<span class="params">self, seq_group: SequenceGroup, num_lookahead_slots: <span class="built_in">int</span> = <span class="number">0</span></span>) -&gt; AllocStatus:</span><br><span class="line">        <span class="comment"># 检查是否有足够的GPU blocks</span></span><br><span class="line">        num_required_blocks = <span class="variable language_">self</span>._get_seq_num_required_blocks(seq_group.get_seqs()[<span class="number">0</span>])</span><br><span class="line">        num_required_blocks += num_lookahead_slots</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.block_allocator.get_num_free_blocks(Device.GPU) &lt; num_required_blocks:</span><br><span class="line">            <span class="keyword">return</span> AllocStatus.NEVER</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> AllocStatus.OK</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">allocate</span>(<span class="params">self, seq_group: SequenceGroup</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;为sequence group分配blocks&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> seq_group.get_seqs():</span><br><span class="line">            block_table = <span class="variable language_">self</span>._allocate_sequence(seq)</span><br><span class="line">            seq.block_table = block_table</span><br></pre></td></tr></table></figure>

<h2 id="SGLang-KV-Cache-管理"><a href="#SGLang-KV-Cache-管理" class="headerlink" title="SGLang KV Cache 管理"></a>SGLang KV Cache 管理</h2><h3 id="整体架构设计-2"><a href="#整体架构设计-2" class="headerlink" title="整体架构设计"></a>整体架构设计</h3><p>SGLang采用<strong>三层内存池架构</strong>，这是三个框架中最复杂和最灵活的设计：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 三层架构设计</span></span><br><span class="line">ReqToTokenPool          <span class="comment"># 第一层：请求到token位置映射</span></span><br><span class="line">├── TokenToKVPoolAllocator    <span class="comment"># 第二层：token到KV pool分配</span></span><br><span class="line">│   ├── PagedTokenToKVPoolAllocator (分页版本)</span><br><span class="line">│   └── SWATokenToKVPoolAllocator (滑动窗口版本)  </span><br><span class="line">└── KVCache                   <span class="comment"># 第三层：物理KV cache存储</span></span><br><span class="line">    ├── MHATokenToKVPool     <span class="comment"># 多头注意力</span></span><br><span class="line">    ├── MLATokenToKVPool     <span class="comment"># MLA注意力  </span></span><br><span class="line">    └── SWAKVPool            <span class="comment"># 滑动窗口注意力</span></span><br></pre></td></tr></table></figure>

<h3 id="1-请求级内存管理"><a href="#1-请求级内存管理" class="headerlink" title="1. 请求级内存管理"></a>1. 请求级内存管理</h3><h4 id="ReqToTokenPool"><a href="#ReqToTokenPool" class="headerlink" title="ReqToTokenPool"></a>ReqToTokenPool</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReqToTokenPool</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size: <span class="built_in">int</span>, max_context_len: <span class="built_in">int</span>, device: <span class="built_in">str</span>, enable_memory_saver: <span class="built_in">bool</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.size = size</span><br><span class="line">        <span class="variable language_">self</span>.max_context_len = max_context_len</span><br><span class="line">        <span class="comment"># 核心张量：[size, max_context_len]，存储token位置</span></span><br><span class="line">        <span class="variable language_">self</span>.req_to_token = torch.zeros((size, max_context_len), </span><br><span class="line">                                       dtype=torch.int32, device=device)</span><br><span class="line">        <span class="variable language_">self</span>.free_slots = <span class="built_in">list</span>(<span class="built_in">range</span>(size))  <span class="comment"># 空闲slot列表</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">alloc</span>(<span class="params">self, need_size: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;分配指定数量的slots&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> need_size &gt; <span class="built_in">len</span>(<span class="variable language_">self</span>.free_slots):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        select_index = <span class="variable language_">self</span>.free_slots[:need_size]</span><br><span class="line">        <span class="variable language_">self</span>.free_slots = <span class="variable language_">self</span>.free_slots[need_size:]</span><br><span class="line">        <span class="keyword">return</span> select_index</span><br></pre></td></tr></table></figure>

<h3 id="2-分页Token分配器"><a href="#2-分页Token分配器" class="headerlink" title="2. 分页Token分配器"></a>2. 分页Token分配器</h3><h4 id="PagedTokenToKVPoolAllocator"><a href="#PagedTokenToKVPoolAllocator" class="headerlink" title="PagedTokenToKVPoolAllocator"></a>PagedTokenToKVPoolAllocator</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PagedTokenToKVPoolAllocator</span>(<span class="title class_ inherited__">BaseTokenToKVPoolAllocator</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size: <span class="built_in">int</span>, page_size: <span class="built_in">int</span>, dtype: torch.dtype, device: <span class="built_in">str</span>, kvcache: KVCache</span>):</span><br><span class="line">        <span class="variable language_">self</span>.size = size</span><br><span class="line">        <span class="variable language_">self</span>.page_size = page_size</span><br><span class="line">        <span class="comment"># 空闲页面列表：每个页面包含page_size个tokens</span></span><br><span class="line">        <span class="variable language_">self</span>.free_pages = torch.arange(<span class="number">1</span>, (size // page_size) + <span class="number">1</span>, device=device)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">alloc_extend</span>(<span class="params">self, prefix_lens: torch.Tensor, seq_lens: torch.Tensor, </span></span><br><span class="line"><span class="params">                    last_loc: torch.Tensor, extend_num_tokens: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;为extend阶段分配内存（使用Triton kernel优化）&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> alloc_extend_kernel(prefix_lens, seq_lens, last_loc, </span><br><span class="line">                                 <span class="variable language_">self</span>.free_pages, extend_num_tokens, <span class="variable language_">self</span>.page_size)</span><br></pre></td></tr></table></figure>

<h4 id="高性能Triton-Kernel"><a href="#高性能Triton-Kernel" class="headerlink" title="高性能Triton Kernel"></a>高性能Triton Kernel</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@triton.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">alloc_extend_kernel</span>(<span class="params">pre_lens_ptr, seq_lens_ptr, last_loc_ptr, free_page_ptr,</span></span><br><span class="line"><span class="params">                       out_indices, ret_values, bs_upper: tl.constexpr, </span></span><br><span class="line"><span class="params">                       page_size: tl.constexpr, max_num_extend_tokens: tl.constexpr</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用Triton实现的高性能内存分配kernel&quot;&quot;&quot;</span></span><br><span class="line">    bid = tl.program_id(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> bid &gt;= bs_upper:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 加载序列信息</span></span><br><span class="line">    pre_len = tl.load(pre_lens_ptr + bid)</span><br><span class="line">    seq_len = tl.load(seq_lens_ptr + bid)</span><br><span class="line">    last_loc = tl.load(last_loc_ptr + bid)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算需要的token数和页数</span></span><br><span class="line">    num_new_tokens = seq_len - pre_len</span><br><span class="line">    num_new_pages = (num_new_tokens + page_size - <span class="number">1</span>) // page_size</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 原子操作分配页面</span></span><br><span class="line">    global_start_page = tl.atomic_add(free_page_ptr, num_new_pages)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成连续的token indices</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_new_tokens):</span><br><span class="line">        page_idx = i // page_size</span><br><span class="line">        page_offset = i % page_size</span><br><span class="line">        token_idx = (global_start_page + page_idx) * page_size + page_offset</span><br><span class="line">        tl.store(out_indices + bid * max_num_extend_tokens + i, token_idx)</span><br></pre></td></tr></table></figure>

<h3 id="3-SWA-Sliding-Window-Attention-支持"><a href="#3-SWA-Sliding-Window-Attention-支持" class="headerlink" title="3. SWA (Sliding Window Attention) 支持"></a>3. SWA (Sliding Window Attention) 支持</h3><h4 id="SWATokenToKVPoolAllocator"><a href="#SWATokenToKVPoolAllocator" class="headerlink" title="SWATokenToKVPoolAllocator"></a>SWATokenToKVPoolAllocator</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SWATokenToKVPoolAllocator</span>(<span class="title class_ inherited__">BaseTokenToKVPoolAllocator</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size: <span class="built_in">int</span>, size_swa: <span class="built_in">int</span>, dtype: torch.dtype, device: <span class="built_in">str</span>, kvcache: SWAKVPool</span>):</span><br><span class="line">        <span class="comment"># 全注意力内存池</span></span><br><span class="line">        <span class="variable language_">self</span>.size_full = size</span><br><span class="line">        <span class="variable language_">self</span>.free_pages_full = torch.arange(<span class="number">1</span>, size + <span class="number">1</span>, device=device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 滑动窗口注意力内存池  </span></span><br><span class="line">        <span class="variable language_">self</span>.size_swa = size_swa</span><br><span class="line">        <span class="variable language_">self</span>.free_pages_swa = torch.arange(<span class="number">1</span>, size_swa + <span class="number">1</span>, device=device)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">alloc</span>(<span class="params">self, need_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;优先从full pool分配，不足时使用swa pool&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> need_size &lt;= <span class="built_in">len</span>(<span class="variable language_">self</span>.free_pages_full):</span><br><span class="line">            <span class="comment"># 从full pool分配</span></span><br><span class="line">            select_index = <span class="variable language_">self</span>.free_pages_full[:need_size]</span><br><span class="line">            <span class="variable language_">self</span>.free_pages_full = <span class="variable language_">self</span>.free_pages_full[need_size:]</span><br><span class="line">            <span class="keyword">return</span> select_index, <span class="string">&quot;full&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> need_size &lt;= <span class="built_in">len</span>(<span class="variable language_">self</span>.free_pages_swa):</span><br><span class="line">            <span class="comment"># 从swa pool分配</span></span><br><span class="line">            select_index = <span class="variable language_">self</span>.free_pages_swa[:need_size]</span><br><span class="line">            <span class="variable language_">self</span>.free_pages_swa = <span class="variable language_">self</span>.free_pages_swa[need_size:]</span><br><span class="line">            <span class="comment"># 转换索引以区分不同池</span></span><br><span class="line">            select_index = <span class="variable language_">self</span>.translate_loc_from_full_to_swa(select_index)</span><br><span class="line">            <span class="keyword">return</span> select_index, <span class="string">&quot;swa&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span>, <span class="string">&quot;none&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="4-物理KV-Cache层"><a href="#4-物理KV-Cache层" class="headerlink" title="4. 物理KV Cache层"></a>4. 物理KV Cache层</h3><h4 id="MLA-Multi-Latent-Attention-支持"><a href="#MLA-Multi-Latent-Attention-支持" class="headerlink" title="MLA (Multi-Latent Attention) 支持"></a>MLA (Multi-Latent Attention) 支持</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLATokenToKVPool</span>(<span class="title class_ inherited__">KVCache</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size: <span class="built_in">int</span>, page_size: <span class="built_in">int</span>, dtype: torch.dtype, </span></span><br><span class="line"><span class="params">                 kv_lora_rank: <span class="built_in">int</span>, qk_rope_head_dim: <span class="built_in">int</span>, layer_num: <span class="built_in">int</span>, device: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="comment"># MLA特殊的KV cache形状：[size, layer_num, kv_lora_rank + qk_rope_head_dim]</span></span><br><span class="line">        <span class="variable language_">self</span>.kv_buffer = torch.empty((size, layer_num, kv_lora_rank + qk_rope_head_dim), </span><br><span class="line">                                    dtype=dtype, device=device)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_mla_kv_buffer</span>(<span class="params">self, layer: RadixAttention, loc: torch.Tensor,</span></span><br><span class="line"><span class="params">                         cache_k_nope: torch.Tensor, cache_k_rope: torch.Tensor</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用Triton kernel高效设置MLA KV buffer&quot;&quot;&quot;</span></span><br><span class="line">        set_mla_kv_buffer_triton(<span class="variable language_">self</span>.kv_buffer, loc, cache_k_nope, cache_k_rope)</span><br></pre></td></tr></table></figure>

<h4 id="高性能MLA-Kernel"><a href="#高性能MLA-Kernel" class="headerlink" title="高性能MLA Kernel"></a>高性能MLA Kernel</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@triton.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_mla_kv_buffer_kernel</span>(<span class="params">kv_buffer_ptr, cache_k_nope_ptr, cache_k_rope_ptr, loc_ptr,</span></span><br><span class="line"><span class="params">                            buffer_stride: tl.constexpr, nope_stride: tl.constexpr, </span></span><br><span class="line"><span class="params">                            rope_stride: tl.constexpr, nope_dim: tl.constexpr, </span></span><br><span class="line"><span class="params">                            rope_dim: tl.constexpr, BLOCK: tl.constexpr</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;MLA KV buffer设置的优化kernel&quot;&quot;&quot;</span></span><br><span class="line">    bid = tl.program_id(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载位置信息</span></span><br><span class="line">    loc = tl.load(loc_ptr + bid)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载nope部分</span></span><br><span class="line">    nope_offset = bid * nope_stride + tl.arange(<span class="number">0</span>, nope_dim)</span><br><span class="line">    cache_k_nope = tl.load(cache_k_nope_ptr + nope_offset)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载rope部分</span></span><br><span class="line">    rope_offset = bid * rope_stride + tl.arange(<span class="number">0</span>, rope_dim)  </span><br><span class="line">    cache_k_rope = tl.load(cache_k_rope_ptr + rope_offset)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 写入KV buffer</span></span><br><span class="line">    buffer_offset = loc * buffer_stride + tl.arange(<span class="number">0</span>, nope_dim + rope_dim)</span><br><span class="line">    combined_kv = tl.join(cache_k_nope, cache_k_rope)</span><br><span class="line">    tl.store(kv_buffer_ptr + buffer_offset, combined_kv)</span><br></pre></td></tr></table></figure>

<h3 id="5-RadixAttention集成"><a href="#5-RadixAttention集成" class="headerlink" title="5. RadixAttention集成"></a>5. RadixAttention集成</h3><h4 id="RadixCache树形结构"><a href="#RadixCache树形结构" class="headerlink" title="RadixCache树形结构"></a>RadixCache树形结构</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RadixCache</span>(<span class="title class_ inherited__">BasePrefixCache</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, req_to_token_pool: ReqToTokenPool, </span></span><br><span class="line"><span class="params">                 token_to_kv_pool: BaseTokenToKVPoolAllocator, page_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.req_to_token_pool = req_to_token_pool</span><br><span class="line">        <span class="variable language_">self</span>.token_to_kv_pool = token_to_kv_pool  </span><br><span class="line">        <span class="variable language_">self</span>.page_size = page_size</span><br><span class="line">        <span class="variable language_">self</span>.root = TreeNode()  <span class="comment"># 根节点</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">match_prefix</span>(<span class="params">self, key: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; MatchResult:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;匹配最长公共前缀&quot;&quot;&quot;</span></span><br><span class="line">        node = <span class="variable language_">self</span>.root</span><br><span class="line">        matched_len = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分页匹配，提高效率</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(key), <span class="variable language_">self</span>.page_size):</span><br><span class="line">            page_key = key[i:i+<span class="variable language_">self</span>.page_size]</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">tuple</span>(page_key) <span class="keyword">in</span> node.children:</span><br><span class="line">                node = node.children[<span class="built_in">tuple</span>(page_key)]</span><br><span class="line">                matched_len += <span class="built_in">len</span>(page_key)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> MatchResult(matched_len, node)</span><br></pre></td></tr></table></figure>

<h2 id="技术对比与深度分析"><a href="#技术对比与深度分析" class="headerlink" title="技术对比与深度分析"></a>技术对比与深度分析</h2><h3 id="1-内存管理策略对比"><a href="#1-内存管理策略对比" class="headerlink" title="1. 内存管理策略对比"></a>1. 内存管理策略对比</h3><table>
<thead>
<tr>
<th>特性</th>
<th>TensorRT-LLM</th>
<th>vLLM</th>
<th>SGLang</th>
</tr>
</thead>
<tbody><tr>
<td><strong>架构复杂度</strong></td>
<td>中等 (分层block)</td>
<td>简单 (模块化)</td>
<td>高 (三层架构)</td>
</tr>
<tr>
<td><strong>内存池设计</strong></td>
<td>按配置分池</td>
<td>CPU&#x2F;GPU分离</td>
<td>多类型Pool</td>
</tr>
<tr>
<td><strong>分页支持</strong></td>
<td>支持 (16,32,64,128)</td>
<td>支持 (可配置)</td>
<td>支持 (可配置)</td>
</tr>
<tr>
<td><strong>SWA支持</strong></td>
<td>支持</td>
<td>部分支持</td>
<td>原生支持</td>
</tr>
<tr>
<td><strong>量化支持</strong></td>
<td>FP8&#x2F;INT4&#x2F;AWQ&#x2F;GPTQ</td>
<td>FP8</td>
<td>基础支持</td>
</tr>
</tbody></table>
<h3 id="2-前缀缓存机制对比"><a href="#2-前缀缓存机制对比" class="headerlink" title="2. 前缀缓存机制对比"></a>2. 前缀缓存机制对比</h3><h4 id="哈希算法性能"><a href="#哈希算法性能" class="headerlink" title="哈希算法性能"></a>哈希算法性能</h4><ul>
<li><strong>TensorRT-LLM</strong>: Wang hash + 64位优化，支持层次化哈希</li>
<li><strong>vLLM</strong>: Python标准hash，简单高效</li>
<li><strong>SGLang</strong>: 分页哈希，树形前缀匹配</li>
</ul>
<h4 id="缓存共享策略"><a href="#缓存共享策略" class="headerlink" title="缓存共享策略"></a>缓存共享策略</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TensorRT-LLM: 引用计数 + 优先级管理</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KVCacheBlock</span> &#123;</span><br><span class="line">    <span class="type">int32_t</span> mRefCount;                  <span class="comment">// 引用计数</span></span><br><span class="line">    executor::RetentionPriority mPriority; <span class="comment">// 优先级</span></span><br><span class="line">    std::chrono::milliseconds mDurationMs; <span class="comment">// 生存时间</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vLLM: 内容哈希 + LRU</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PrefixCachingBlockAllocator</span> &#123;</span><br><span class="line">    _cached_blocks: <span class="type">Dict</span>[PrefixHash, BlockId]  <span class="comment"># 哈希到块ID映射</span></span><br><span class="line">    _touched_blocks: <span class="type">Set</span>[BlockId]              <span class="comment"># 调度器触及的块</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGLang: 树形结构 + 分页匹配</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TreeNode</span> &#123;</span><br><span class="line">    children: defaultdict(TreeNode)     <span class="comment"># 子节点映射</span></span><br><span class="line">    value: <span class="type">Optional</span>[torch.Tensor]       <span class="comment"># 缓存值</span></span><br><span class="line">    last_access_time: <span class="built_in">float</span>            <span class="comment"># 最后访问时间</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-性能优化技术"><a href="#3-性能优化技术" class="headerlink" title="3. 性能优化技术"></a>3. 性能优化技术</h3><h4 id="内存访问优化"><a href="#内存访问优化" class="headerlink" title="内存访问优化"></a>内存访问优化</h4><ol>
<li><p><strong>TensorRT-LLM</strong>:</p>
<ul>
<li>CUDA kernel优化的paged attention</li>
<li>多级缓存 (primary&#x2F;secondary)</li>
<li>Fabric memory支持高速传输</li>
</ul>
</li>
<li><p><strong>vLLM</strong>: </p>
<ul>
<li>Triton kernel实现</li>
<li>FlashAttention集成</li>
<li>多后端支持 (CUDA&#x2F;ROCm&#x2F;CPU)</li>
</ul>
</li>
<li><p><strong>SGLang</strong>:</p>
<ul>
<li>最多Triton kernel优化</li>
<li>专门的MLA&#x2F;SWA支持  </li>
<li>硬件特定优化 (Ascend NPU)</li>
</ul>
</li>
</ol>
<h4 id="并发和同步"><a href="#并发和同步" class="headerlink" title="并发和同步"></a>并发和同步</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TensorRT-LLM: 细粒度锁</span></span><br><span class="line">std::mutex mAllocatedBlocksPerSeqMutex;</span><br><span class="line">std::condition_variable mCondVar;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调度期间的引用计数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">KVCacheBlock::startScheduling</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    mSchedulingRefCount = mRefCount;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vLLM: 简化的状态管理</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BlockTracker</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">enable</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> <span class="variable language_">self</span>.active</span><br><span class="line">        <span class="variable language_">self</span>.active = <span class="literal">True</span></span><br><span class="line">        <span class="variable language_">self</span>.reset()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGLang: 分组释放优化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">free_group_begin</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="variable language_">self</span>.is_not_in_free_group = <span class="literal">False</span></span><br><span class="line">    <span class="variable language_">self</span>.free_group = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">free_group_end</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.free_group:</span><br><span class="line">        <span class="variable language_">self</span>.free(torch.cat(<span class="variable language_">self</span>.free_group))  <span class="comment"># 批量释放</span></span><br></pre></td></tr></table></figure>

<h2 id="性能优化策略"><a href="#性能优化策略" class="headerlink" title="性能优化策略"></a>性能优化策略</h2><h3 id="1-内存分配优化"><a href="#1-内存分配优化" class="headerlink" title="1. 内存分配优化"></a>1. 内存分配优化</h3><h4 id="TensorRT-LLM优化要点"><a href="#TensorRT-LLM优化要点" class="headerlink" title="TensorRT-LLM优化要点"></a>TensorRT-LLM优化要点</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 预分配策略 - 避免运行时分配</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">WindowBlockManager::allocatePools</span><span class="params">(<span class="type">bool</span> useUvm)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 预分配所有内存池</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; pool : mPools) &#123;</span><br><span class="line">        pool.primaryPtr = mBufferManager.<span class="built_in">gpuSync</span>(cacheShape, poolDtype);</span><br><span class="line">        <span class="keyword">if</span> (mNumSecondaryBlocks &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            pool.secondaryPtr = BufferManager::<span class="built_in">pinned</span>(cacheShapeOffload, poolDtype);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 智能驱逐策略</span></span><br><span class="line"><span class="function">BlockPtr <span class="title">getFreeBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 优先级队列 + LRU</span></span><br><span class="line">    <span class="keyword">auto</span> [block, canOffload] = mEvictionPolicy-&gt;<span class="built_in">getFreeBlock</span>(kPrimaryLevel);</span><br><span class="line">    <span class="keyword">if</span> (canOffload &amp;&amp; needOffload) &#123;</span><br><span class="line">        <span class="comment">// 异步offload到secondary memory</span></span><br><span class="line">        mTransferManager-&gt;<span class="built_in">offload</span>(block, offloadBlock, mPools);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> block;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="vLLM优化要点"><a href="#vLLM优化要点" class="headerlink" title="vLLM优化要点"></a>vLLM优化要点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 延迟计算 - 仅在需要时计算哈希</span></span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">content_hash</span>(<span class="params">self</span>) -&gt; <span class="type">Optional</span>[<span class="built_in">int</span>]:</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>._content_hash <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="variable language_">self</span>.token_ids:</span><br><span class="line">        <span class="variable language_">self</span>._content_hash = <span class="variable language_">self</span>.hash_block_tokens(...)</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>._content_hash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 设备感知分配</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">allocate_mutable_block</span>(<span class="params">self, device: Device</span>) -&gt; Block:</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>._allocators[device].allocate_mutable_block(...)</span><br></pre></td></tr></table></figure>

<h4 id="SGLang优化要点"><a href="#SGLang优化要点" class="headerlink" title="SGLang优化要点"></a>SGLang优化要点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Triton kernel加速</span></span><br><span class="line"><span class="meta">@triton.jit  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">alloc_extend_kernel</span>(<span class="params">...</span>):</span><br><span class="line">    <span class="comment"># GPU并行分配，避免CPU-GPU同步</span></span><br><span class="line">    global_start_page = tl.atomic_add(free_page_ptr, num_new_pages)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 分组操作减少开销</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">free_group_end</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.free_group:</span><br><span class="line">        <span class="variable language_">self</span>.free(torch.cat(<span class="variable language_">self</span>.free_group))  <span class="comment"># 批量释放</span></span><br></pre></td></tr></table></figure>

<h3 id="2-缓存命中率优化"><a href="#2-缓存命中率优化" class="headerlink" title="2. 缓存命中率优化"></a>2. 缓存命中率优化</h3><h4 id="前缀匹配算法"><a href="#前缀匹配算法" class="headerlink" title="前缀匹配算法"></a>前缀匹配算法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGLang: 分页前缀匹配</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_key_match_paged</span>(<span class="params">key0: <span class="type">List</span>, key1: <span class="type">List</span>, page_size: <span class="built_in">int</span></span>):</span><br><span class="line">    min_len = <span class="built_in">min</span>(<span class="built_in">len</span>(key0), <span class="built_in">len</span>(key1))</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; min_len:</span><br><span class="line">        <span class="keyword">if</span> key0[i : i + page_size] != key1[i : i + page_size]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i += page_size</span><br><span class="line">    <span class="keyword">return</span> i</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorRT-LLM: 部分块复用</span></span><br><span class="line">std::<span class="built_in">tuple</span>&lt;<span class="built_in">bool</span>, SizeType32, BlockPtr&gt; findMatchingBlock(</span><br><span class="line">    BlockKey const&amp; blockKey, <span class="built_in">bool</span> enablePartialReuse) &#123;</span><br><span class="line">    <span class="keyword">if</span> (enablePartialReuse) &#123;</span><br><span class="line">        // 查找最佳部分匹配</span><br><span class="line">        <span class="keyword">for</span> (auto const&amp; [key, block] : mNextBlocks) &#123;</span><br><span class="line">            SizeType32 numMatched = key.partialMatch(blockKey);</span><br><span class="line">            <span class="keyword">if</span> (numMatched &gt; bestNumMatched) &#123;</span><br><span class="line">                bestNumMatched = numMatched;</span><br><span class="line">                bestBlock = block;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="缓存替换策略"><a href="#缓存替换策略" class="headerlink" title="缓存替换策略"></a>缓存替换策略</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TensorRT-LLM: 多因素LRU</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LRUEvictionPolicy</span> &#123;</span><br><span class="line">    <span class="comment">// 考虑优先级、访问时间、引用计数</span></span><br><span class="line">    std::vector&lt;std::vector&lt;FreeBlocksQueue&gt;&gt; mFreeQueues; <span class="comment">// [level][priority]</span></span><br><span class="line">    </span><br><span class="line">    <span class="function">BlockPtr <span class="title">getLRUBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 优先级 &gt; 最后访问时间 &gt; 引用计数</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> priority = kMinPriority; priority &lt;= kMaxPriority; ++priority) &#123;</span><br><span class="line">            <span class="keyword">auto</span>&amp; queue = mFreeQueues[level][priority];</span><br><span class="line">            <span class="keyword">if</span> (!queue.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">                <span class="keyword">return</span> queue.<span class="built_in">front</span>(); <span class="comment">// LRU order</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h3 id="3-内存使用优化"><a href="#3-内存使用优化" class="headerlink" title="3. 内存使用优化"></a>3. 内存使用优化</h3><h4 id="动态内存管理"><a href="#动态内存管理" class="headerlink" title="动态内存管理"></a>动态内存管理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGLang: 动态SWA池切换</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SWATokenToKVPoolAllocator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">alloc</span>(<span class="params">self, need_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="keyword">if</span> need_size &lt;= <span class="built_in">len</span>(<span class="variable language_">self</span>.free_pages_full):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.free_pages_full[:need_size], <span class="string">&quot;full&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> need_size &lt;= <span class="built_in">len</span>(<span class="variable language_">self</span>.free_pages_swa):</span><br><span class="line">            <span class="comment"># 动态切换到SWA池</span></span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.translate_loc_from_full_to_swa(</span><br><span class="line">                <span class="variable language_">self</span>.free_pages_swa[:need_size]), <span class="string">&quot;swa&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="内存碎片减少"><a href="#内存碎片减少" class="headerlink" title="内存碎片减少"></a>内存碎片减少</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TensorRT-LLM: 连续块分配</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">addSequence</span><span class="params">(GenerationRequest&amp; sequence, SizeType32 inputLength)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 尽量分配连续的blocks以减少碎片</span></span><br><span class="line">    <span class="keyword">auto</span> blockedUniqueTokens = <span class="built_in">chopVectorIntoBlocks</span>(uniqueTokens, inputLength<span class="number">-1</span>, mTokensPerBlock);</span><br><span class="line">    <span class="keyword">auto</span> prepopulatedPromptLen = <span class="built_in">loadOrAllocateBlocks</span>(blockKeys, numContextBlocks, sequence);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="总结与建议"><a href="#总结与建议" class="headerlink" title="总结与建议"></a>总结与建议</h2><h3 id="各框架优势总结"><a href="#各框架优势总结" class="headerlink" title="各框架优势总结"></a>各框架优势总结</h3><h4 id="TensorRT-LLM"><a href="#TensorRT-LLM" class="headerlink" title="TensorRT-LLM"></a>TensorRT-LLM</h4><p><strong>优势</strong>:</p>
<ul>
<li>生产级稳定性和性能</li>
<li>完善的量化支持</li>
<li>优秀的CUDA kernel优化</li>
<li>企业级特性 (优先级、过期时间等)</li>
</ul>
<p><strong>适用场景</strong>:</p>
<ul>
<li>大规模生产部署</li>
<li>高性能要求的应用</li>
<li>NVIDIA GPU集群环境</li>
</ul>
<h4 id="vLLM"><a href="#vLLM" class="headerlink" title="vLLM"></a>vLLM</h4><p><strong>优势</strong>:</p>
<ul>
<li>代码简洁，易于理解和扩展</li>
<li>良好的社区支持</li>
<li>灵活的后端支持</li>
<li>快速迭代和新特性集成</li>
</ul>
<p><strong>适用场景</strong>:</p>
<ul>
<li>研究和原型开发</li>
<li>多样化硬件环境</li>
<li>需要快速集成新特性</li>
</ul>
<h4 id="SGLang"><a href="#SGLang" class="headerlink" title="SGLang"></a>SGLang</h4><p><strong>优势</strong>:</p>
<ul>
<li>最先进的内存管理架构</li>
<li>原生多模态支持</li>
<li>尖端优化技术 (MLA, SWA等)</li>
<li>硬件异构支持</li>
</ul>
<p><strong>适用场景</strong>:</p>
<ul>
<li>前沿研究项目  </li>
<li>复杂attention模式需求</li>
<li>异构硬件环境</li>
</ul>
<h3 id="技术发展趋势"><a href="#技术发展趋势" class="headerlink" title="技术发展趋势"></a>技术发展趋势</h3><ol>
<li><strong>架构融合</strong>: 各框架正在互相借鉴优势特性</li>
<li><strong>硬件特化</strong>: 针对不同硬件的专门优化</li>
<li><strong>智能调度</strong>: AI驱动的内存管理和调度策略</li>
<li><strong>异构计算</strong>: CPU&#x2F;GPU&#x2F;NPU协同的KV cache管理</li>
</ol>
<h3 id="选择建议"><a href="#选择建议" class="headerlink" title="选择建议"></a>选择建议</h3><table>
<thead>
<tr>
<th>需求</th>
<th>推荐框架</th>
<th>理由</th>
</tr>
</thead>
<tbody><tr>
<td>生产部署</td>
<td>TensorRT-LLM</td>
<td>稳定性和性能最佳</td>
</tr>
<tr>
<td>研究开发</td>
<td>vLLM</td>
<td>开发效率和扩展性好</td>
</tr>
<tr>
<td>前沿探索</td>
<td>SGLang</td>
<td>最新技术和架构创新</td>
</tr>
<tr>
<td>多模态应用</td>
<td>SGLang</td>
<td>原生多模态支持</td>
</tr>
<tr>
<td>异构硬件</td>
<td>SGLang &gt; vLLM &gt; TensorRT-LLM</td>
<td>硬件支持广度</td>
</tr>
</tbody></table>
<p>选择框架时应综合考虑性能需求、开发资源、部署环境和长期维护等因素。 </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/14/System/LLM_KVCache_%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/" data-id="cme16yl09001hwyon0c2thsex" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/07/14/System/LLM_Serving_Framework_Comparison/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2025/07/10/Programming/CPP/modern%20cpu/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/System/">System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Test/">Test</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Communication/" rel="tag">Communication</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Compiler/" rel="tag">Compiler</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Graph/" rel="tag">Computational Graph</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepSeek/" rel="tag">DeepSeek</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distributed/" rel="tag">Distributed</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MoE/" rel="tag">MoE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization/" rel="tag">Optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Performance/" rel="tag">Performance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SGLang/" rel="tag">SGLang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/" rel="tag">System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/" rel="tag">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blues/" rel="tag">blues</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/career/" rel="tag">career</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/charts/" rel="tag">charts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/consensus/" rel="tag">consensus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/data-parallelism/" rel="tag">data-parallelism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/distributed-systems/" rel="tag">distributed-systems</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/experience/" rel="tag">experience</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/expert-parallelism/" rel="tag">expert-parallelism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/guitar/" rel="tag">guitar</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/harmony/" rel="tag">harmony</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/licks/" rel="tag">licks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/" rel="tag">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mermaid/" rel="tag">mermaid</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/raft/" rel="tag">raft</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rhythm/" rel="tag">rhythm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/test/" rel="tag">test</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/Communication/" style="font-size: 10px;">Communication</a> <a href="/tags/Compiler/" style="font-size: 10px;">Compiler</a> <a href="/tags/Computational-Graph/" style="font-size: 10px;">Computational Graph</a> <a href="/tags/DeepSeek/" style="font-size: 10px;">DeepSeek</a> <a href="/tags/Distributed/" style="font-size: 10px;">Distributed</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/MoE/" style="font-size: 10px;">MoE</a> <a href="/tags/Optimization/" style="font-size: 10px;">Optimization</a> <a href="/tags/Performance/" style="font-size: 10px;">Performance</a> <a href="/tags/SGLang/" style="font-size: 10px;">SGLang</a> <a href="/tags/System/" style="font-size: 15px;">System</a> <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/blues/" style="font-size: 20px;">blues</a> <a href="/tags/career/" style="font-size: 10px;">career</a> <a href="/tags/charts/" style="font-size: 10px;">charts</a> <a href="/tags/consensus/" style="font-size: 10px;">consensus</a> <a href="/tags/data-parallelism/" style="font-size: 10px;">data-parallelism</a> <a href="/tags/distributed-systems/" style="font-size: 20px;">distributed-systems</a> <a href="/tags/experience/" style="font-size: 10px;">experience</a> <a href="/tags/expert-parallelism/" style="font-size: 10px;">expert-parallelism</a> <a href="/tags/guitar/" style="font-size: 20px;">guitar</a> <a href="/tags/harmony/" style="font-size: 10px;">harmony</a> <a href="/tags/licks/" style="font-size: 10px;">licks</a> <a href="/tags/life/" style="font-size: 10px;">life</a> <a href="/tags/llm/" style="font-size: 15px;">llm</a> <a href="/tags/mermaid/" style="font-size: 10px;">mermaid</a> <a href="/tags/raft/" style="font-size: 10px;">raft</a> <a href="/tags/rhythm/" style="font-size: 10px;">rhythm</a> <a href="/tags/test/" style="font-size: 10px;">test</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/08/">August 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/05/">May 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/08/03/Plans/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%9A%BE%E9%A2%98/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/08/02/System/LLM%20Serving%20Configs/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/31/System/cuda-kernel/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/31/Programming/CPP/constexpr/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/30/System/%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/%E5%BA%8F%E5%88%97%E5%B9%B6%E8%A1%8C/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






<!-- Mermaid Support -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
<script>
  mermaid.initialize({
    startOnLoad: true,
    theme: 'default',
    securityLevel: 'loose',
    themeVariables: {
      primaryColor: '#0f4c75',
      primaryTextColor: '#fff',
      primaryBorderColor: '#0f4c75',
      lineColor: '#0f4c75',
      secondaryColor: '#006ba6',
      tertiaryColor: '#fff'
    }
  });
</script>
  </div>
</body>
</html>