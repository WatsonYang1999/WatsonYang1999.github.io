<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="vLLM Paged Attention 架构与实现深度分析1. 概述Paged Attention 是 vLLM 的核心创新，它通过将 KV 缓存组织成固定大小的页（block）来实现高效的内存管理。这种设计大幅提升了 GPU 内存利用率，并支持动态序列长度处理。 2. 核心组件架构2.1 PagedAttention 类层次结构1234PagedAttention (主要实现)├── HPUP">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2025/07/16/System/vLLM_PagedAttention_%E5%AE%9E%E7%8E%B0%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="vLLM Paged Attention 架构与实现深度分析1. 概述Paged Attention 是 vLLM 的核心创新，它通过将 KV 缓存组织成固定大小的页（block）来实现高效的内存管理。这种设计大幅提升了 GPU 内存利用率，并支持动态序列长度处理。 2. 核心组件架构2.1 PagedAttention 类层次结构1234PagedAttention (主要实现)├── HPUP">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-07-16T02:11:51.911Z">
<meta property="article:modified_time" content="2025-07-16T09:34:06.104Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

  
  <!-- Mermaid -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: 'default',
      securityLevel: 'loose',
      themeVariables: {
        primaryColor: '#0f4c75',
        primaryTextColor: '#fff',
        primaryBorderColor: '#0f4c75',
        lineColor: '#0f4c75',
        secondaryColor: '#006ba6',
        tertiaryColor: '#fff'
      }
    });
  </script>
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-System/vLLM_PagedAttention_实现详细分析" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/16/System/vLLM_PagedAttention_%E5%AE%9E%E7%8E%B0%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/" class="article-date">
  <time class="dt-published" datetime="2025-07-16T02:11:51.911Z" itemprop="datePublished">2025-07-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="vLLM-Paged-Attention-架构与实现深度分析"><a href="#vLLM-Paged-Attention-架构与实现深度分析" class="headerlink" title="vLLM Paged Attention 架构与实现深度分析"></a>vLLM Paged Attention 架构与实现深度分析</h1><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>Paged Attention 是 vLLM 的核心创新，它通过将 KV 缓存组织成固定大小的页（block）来实现高效的内存管理。这种设计大幅提升了 GPU 内存利用率，并支持动态序列长度处理。</p>
<h2 id="2-核心组件架构"><a href="#2-核心组件架构" class="headerlink" title="2. 核心组件架构"></a>2. 核心组件架构</h2><h3 id="2-1-PagedAttention-类层次结构"><a href="#2-1-PagedAttention-类层次结构" class="headerlink" title="2.1 PagedAttention 类层次结构"></a>2.1 PagedAttention 类层次结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PagedAttention (主要实现)</span><br><span class="line">├── HPUPagedAttention (HPU 平台特化)</span><br><span class="line">├── AITERPagedAttention (ROCm AITER 优化)</span><br><span class="line">└── 平台特定实现</span><br></pre></td></tr></table></figure>

<h3 id="2-2-Paged-Attention-系统架构图"><a href="#2-2-Paged-Attention-系统架构图" class="headerlink" title="2.2 Paged Attention 系统架构图"></a>2.2 Paged Attention 系统架构图</h3><pre class="mermaid">graph TB
    subgraph "Paged Attention 系统架构"
        subgraph "内存管理层"
            BM[BlockManager<br/>块管理器]
            BA[BlockAllocator<br/>块分配器]
            BT[BlockTable<br/>块表]
            BP[BlockPool<br/>块池]
        end
        
        subgraph "计算层"
            PA[PagedAttention<br/>分页注意力]
            PA_V1[PagedAttention V1<br/>短序列优化]
            PA_V2[PagedAttention V2<br/>长序列优化]
            PA_Prefix[Prefix Attention<br/>前缀注意力]
        end
        
        subgraph "存储层"
            KV_Cache[KV Cache<br/>键值缓存]
            Key_Blocks[Key Blocks<br/>键块]
            Value_Blocks[Value Blocks<br/>值块]
            Block_Mapping[Block Mapping<br/>块映射]
        end
        
        subgraph "平台特化层"
            CUDA_Ops[CUDA Operations<br/>CUDA操作]
            ROCm_Ops[ROCm Operations<br/>ROCm操作]
            HPU_Ops[HPU Operations<br/>HPU操作]
        end
    end
    
    %% 连接关系
    BM --> BA
    BA --> BP
    BM --> BT
    BT --> Block_Mapping
    
    PA --> PA_V1
    PA --> PA_V2
    PA --> PA_Prefix
    
    PA_V1 --> CUDA_Ops
    PA_V2 --> CUDA_Ops
    PA_Prefix --> CUDA_Ops
    
    PA_V1 --> ROCm_Ops
    PA_V2 --> ROCm_Ops
    
    PA_V1 --> HPU_Ops
    PA_V2 --> HPU_Ops
    
    KV_Cache --> Key_Blocks
    KV_Cache --> Value_Blocks
    Block_Mapping --> Key_Blocks
    Block_Mapping --> Value_Blocks
    
    BT --> KV_Cache
    PA --> KV_Cache</pre>

<h3 id="2-4-关键数据结构"><a href="#2-4-关键数据结构" class="headerlink" title="2.4 关键数据结构"></a>2.4 关键数据结构</h3><h4 id="PagedAttentionMetadata"><a href="#PagedAttentionMetadata" class="headerlink" title="PagedAttentionMetadata"></a>PagedAttentionMetadata</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PagedAttentionMetadata</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;PagedAttention 的元数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># (batch_size,) 每个序列的长度（已见过的所有 token）</span></span><br><span class="line">    seq_lens_tensor: <span class="type">Optional</span>[torch.Tensor]</span><br><span class="line">    <span class="comment"># 批次中最大解码序列长度，若为 prefill-only 批次则为 0</span></span><br><span class="line">    max_decode_seq_len: <span class="built_in">int</span></span><br><span class="line">    <span class="comment"># (batch_size, max_blocks_per_seq) 每个序列的块地址</span></span><br><span class="line">    <span class="comment"># 例如 [0, 1, 2] 表示 token 存储在 KV 缓存的第 0、1、2 块中</span></span><br><span class="line">    block_tables: <span class="type">Optional</span>[torch.Tensor]</span><br></pre></td></tr></table></figure>

<h3 id="2-5-内存布局设计"><a href="#2-5-内存布局设计" class="headerlink" title="2.5 内存布局设计"></a>2.5 内存布局设计</h3><h4 id="KV-缓存形状"><a href="#KV-缓存形状" class="headerlink" title="KV 缓存形状"></a>KV 缓存形状</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_kv_cache_shape</span>(<span class="params">num_blocks: <span class="built_in">int</span>, block_size: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                       num_kv_heads: <span class="built_in">int</span>, head_size: <span class="built_in">int</span></span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">int</span>, ...]:</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">2</span>, num_blocks, block_size * num_kv_heads * head_size)</span><br></pre></td></tr></table></figure>

<p>KV 缓存被组织为：</p>
<ul>
<li><strong>维度 0</strong>: K&#x2F;V 分离 (0 &#x3D; Key, 1 &#x3D; Value)</li>
<li><strong>维度 1</strong>: 物理块数量</li>
<li><strong>维度 2</strong>: 每块的实际存储 (block_size × num_kv_heads × head_size)</li>
</ul>
<h4 id="内存布局与块管理图"><a href="#内存布局与块管理图" class="headerlink" title="内存布局与块管理图"></a>内存布局与块管理图</h4><pre class="mermaid">graph TB
    subgraph "KV Cache 内存布局"
        subgraph "物理内存视图"
            PM[Physical Memory<br/>物理内存]
            Block_0[Block 0<br/>块 0]
            Block_1[Block 1<br/>块 1]
            Block_2[Block 2<br/>块 2]
            Block_N[Block N<br/>块 N]
        end
        
        subgraph "逻辑视图"
            KV_Cache[KV Cache<br/>键值缓存]
            Key_Cache[Key Cache<br/>键缓存]
            Value_Cache[Value Cache<br/>值缓存]
        end
        
        subgraph "块表映射"
            BT[Block Table<br/>块表]
            Seq_1[Sequence 1<br/>序列 1]
            Seq_2[Sequence 2<br/>序列 2]
            Seq_3[Sequence 3<br/>序列 3]
        end
        
        subgraph "序列到块的映射"
            Token_0[Token 0-15<br/>令牌 0-15]
            Token_1[Token 16-31<br/>令牌 16-31]
            Token_2[Token 32-47<br/>令牌 32-47]
        end
    end
    
    %% 物理内存连接
    PM --> Block_0
    PM --> Block_1
    PM --> Block_2
    PM --> Block_N
    
    %% 逻辑视图连接
    KV_Cache --> Key_Cache
    KV_Cache --> Value_Cache
    
    %% 块表映射
    BT --> Seq_1
    BT --> Seq_2
    BT --> Seq_3
    
    %% 序列到块的映射
    Seq_1 --> Token_0
    Seq_1 --> Token_1
    Seq_2 --> Token_1
    Seq_2 --> Token_2
    Seq_3 --> Token_0
    
    %% 块到物理内存的映射
    Token_0 --> Block_0
    Token_1 --> Block_1
    Token_2 --> Block_2
    
    %% 样式
    classDef blockStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef cacheStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef tableStyle fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    
    class Block_0,Block_1,Block_2,Block_N blockStyle
    class Key_Cache,Value_Cache,KV_Cache cacheStyle
    class BT,Seq_1,Seq_2,Seq_3 tableStyle</pre>

<h2 id="3-核心算法实现"><a href="#3-核心算法实现" class="headerlink" title="3. 核心算法实现"></a>3. 核心算法实现</h2><h3 id="3-1-KV-缓存分割与重塑"><a href="#3-1-KV-缓存分割与重塑" class="headerlink" title="3.1 KV 缓存分割与重塑"></a>3.1 KV 缓存分割与重塑</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_kv_cache</span>(<span class="params">kv_cache: torch.Tensor, num_kv_heads: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                   head_size: <span class="built_in">int</span></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将 KV 缓存分割为 Key 和 Value 缓存&quot;&quot;&quot;</span></span><br><span class="line">    x = <span class="number">16</span> // kv_cache.element_size()  <span class="comment"># 内存对齐优化</span></span><br><span class="line">    num_blocks = kv_cache.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Key 缓存重塑：支持量化存储</span></span><br><span class="line">    key_cache = kv_cache[<span class="number">0</span>].view(num_blocks, num_kv_heads, head_size // x, -<span class="number">1</span>, x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Value 缓存重塑：连续存储</span></span><br><span class="line">    value_cache = kv_cache[<span class="number">1</span>].view(num_blocks, num_kv_heads, head_size, -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> key_cache, value_cache</span><br></pre></td></tr></table></figure>

<h3 id="3-2-写入分页缓存"><a href="#3-2-写入分页缓存" class="headerlink" title="3.2 写入分页缓存"></a>3.2 写入分页缓存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_to_paged_cache</span>(<span class="params">key: torch.Tensor, value: torch.Tensor,</span></span><br><span class="line"><span class="params">                         key_cache: torch.Tensor, value_cache: torch.Tensor,</span></span><br><span class="line"><span class="params">                         slot_mapping: torch.Tensor, kv_cache_dtype: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">                         k_scale: torch.Tensor, v_scale: torch.Tensor</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将新的 KV 写入分页缓存&quot;&quot;&quot;</span></span><br><span class="line">    ops.reshape_and_cache(key, value, key_cache, value_cache, </span><br><span class="line">                          slot_mapping.flatten(), kv_cache_dtype, </span><br><span class="line">                          k_scale, v_scale)</span><br></pre></td></tr></table></figure>

<h3 id="3-3-前向解码过程"><a href="#3-3-前向解码过程" class="headerlink" title="3.3 前向解码过程"></a>3.3 前向解码过程</h3><h4 id="版本选择策略"><a href="#版本选择策略" class="headerlink" title="版本选择策略"></a>版本选择策略</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_decode</span>(<span class="params">...</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码阶段的 Paged Attention 计算&quot;&quot;&quot;</span></span><br><span class="line">    max_num_partitions = (max_seq_len + _PARTITION_SIZE - <span class="number">1</span>) // _PARTITION_SIZE</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 启发式版本选择</span></span><br><span class="line">    use_v1 = (max_seq_len &lt;= <span class="number">8192</span> </span><br><span class="line">              <span class="keyword">and</span> (max_num_partitions == <span class="number">1</span> <span class="keyword">or</span> num_seqs * num_heads &gt; <span class="number">512</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> use_v1:</span><br><span class="line">        <span class="comment"># 使用 PagedAttention V1 - 适合短序列或高并发</span></span><br><span class="line">        ops.paged_attention_v1(...)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 使用 PagedAttention V2 - 适合长序列，避免共享内存不足</span></span><br><span class="line">        ops.paged_attention_v2(...)</span><br></pre></td></tr></table></figure>

<h4 id="PagedAttention-V1-vs-V2"><a href="#PagedAttention-V1-vs-V2" class="headerlink" title="PagedAttention V1 vs V2"></a>PagedAttention V1 vs V2</h4><p><strong>V1 特点</strong>:</p>
<ul>
<li>单次计算完成</li>
<li>适合短序列 (≤8192 tokens)</li>
<li>高并发时性能更好</li>
<li>共享内存使用较少</li>
</ul>
<p><strong>V2 特点</strong>:</p>
<ul>
<li>分片计算 + 规约操作</li>
<li>适合长序列 (&gt;8192 tokens)</li>
<li>避免共享内存溢出</li>
<li>需要额外的临时缓冲区</li>
</ul>
<h3 id="3-4-V2-算法详细实现"><a href="#3-4-V2-算法详细实现" class="headerlink" title="3.4 V2 算法详细实现"></a>3.4 V2 算法详细实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># V2 需要的临时缓冲区</span></span><br><span class="line">tmp_output = torch.empty(</span><br><span class="line">    size=(num_seqs, num_heads, max_num_partitions, head_size),</span><br><span class="line">    dtype=output.dtype, device=output.device)</span><br><span class="line">exp_sums = torch.empty(</span><br><span class="line">    size=(num_seqs, num_heads, max_num_partitions),</span><br><span class="line">    dtype=torch.float32, device=output.device)</span><br><span class="line">max_logits = torch.empty_like(exp_sums)</span><br><span class="line"></span><br><span class="line">ops.paged_attention_v2(output, exp_sums, max_logits, tmp_output, ...)</span><br></pre></td></tr></table></figure>

<h3 id="3-5-Paged-Attention-计算流程图"><a href="#3-5-Paged-Attention-计算流程图" class="headerlink" title="3.5 Paged Attention 计算流程图"></a>3.5 Paged Attention 计算流程图</h3><pre class="mermaid">flowchart TD
    Start([开始 Paged Attention]) --> Check_Version{选择算法版本}
    
    Check_Version -->|短序列 ≤8192| V1[PagedAttention V1<br/>单次计算]
    Check_Version -->|长序列 >8192| V2[PagedAttention V2<br/>分片计算]
    
    subgraph "V1 算法流程"
        V1 --> V1_Init[初始化输出张量]
        V1_Init --> V1_Compute[单次注意力计算]
        V1_Compute --> V1_Finish[完成计算]
    end
    
    subgraph "V2 算法流程"
        V2 --> V2_Init[初始化临时缓冲区]
        V2_Init --> V2_Partition[分片计算]
        V2_Partition --> V2_Reduce[规约操作]
        V2_Reduce --> V2_Finish[完成计算]
    end
    
    subgraph "共同步骤"
        V1_Finish --> Common_Steps[后处理步骤]
        V2_Finish --> Common_Steps
        Common_Steps --> Output_Process[输出处理]
        Output_Process --> End([结束])
    end
    
    subgraph "内存管理"
        Block_Alloc[块分配]
        Cache_Write[缓存写入]
        Block_Table[块表更新]
    end
    
    V1_Compute --> Block_Alloc
    V2_Partition --> Block_Alloc
    Block_Alloc --> Cache_Write
    Cache_Write --> Block_Table
    
    %% 样式定义
    classDef v1Style fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef v2Style fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef commonStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef memoryStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class V1,V1_Init,V1_Compute,V1_Finish v1Style
    class V2,V2_Init,V2_Partition,V2_Reduce,V2_Finish v2Style
    class Common_Steps,Output_Process commonStyle
    class Block_Alloc,Cache_Write,Block_Table memoryStyle</pre>

<h2 id="4-内存管理系统"><a href="#4-内存管理系统" class="headerlink" title="4. 内存管理系统"></a>4. 内存管理系统</h2><h3 id="4-1-Block-分配器架构"><a href="#4-1-Block-分配器架构" class="headerlink" title="4.1 Block 分配器架构"></a>4.1 Block 分配器架构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DeviceAwareBlockAllocator (设备感知分配器)</span><br><span class="line">├── CpuGpuBlockAllocator (CPU/GPU 联合分配器)</span><br><span class="line">    ├── PrefixCachingBlockAllocator (前缀缓存分配器)</span><br><span class="line">    └── NaiveBlockAllocator (朴素分配器)</span><br></pre></td></tr></table></figure>

<h3 id="4-2-BlockSpaceManager"><a href="#4-2-BlockSpaceManager" class="headerlink" title="4.2 BlockSpaceManager"></a>4.2 BlockSpaceManager</h3><h4 id="核心职责"><a href="#核心职责" class="headerlink" title="核心职责"></a>核心职责</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttnBlockSpaceManager</span>(<span class="title class_ inherited__">BlockSpaceManager</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;管理 KV 缓存的块空间分配器</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    核心功能：</span></span><br><span class="line"><span class="string">    - 分配、交换、释放内存块</span></span><br><span class="line"><span class="string">    - 支持前缀缓存、fork/copy-on-write</span></span><br><span class="line"><span class="string">    - 滑动窗口内存分配</span></span><br><span class="line"><span class="string">    - Lookahead slot 管理（用于投机解码）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block_size: <span class="built_in">int</span>, num_gpu_blocks: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                 num_cpu_blocks: <span class="built_in">int</span>, watermark: <span class="built_in">float</span> = <span class="number">0.01</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.block_allocator = CpuGpuBlockAllocator.create(</span><br><span class="line">            allocator_type=<span class="string">&quot;prefix_caching&quot;</span> <span class="keyword">if</span> enable_caching <span class="keyword">else</span> <span class="string">&quot;naive&quot;</span>,</span><br><span class="line">            num_gpu_blocks=num_gpu_blocks,</span><br><span class="line">            num_cpu_blocks=num_cpu_blocks,</span><br><span class="line">            block_size=block_size)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-BlockTable-实现"><a href="#4-3-BlockTable-实现" class="headerlink" title="4.3 BlockTable 实现"></a>4.3 BlockTable 实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BlockTable</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;管理特定序列的块表&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">allocate</span>(<span class="params">self, token_ids: <span class="type">List</span>[<span class="built_in">int</span>], device: Device = Device.GPU</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;为 token 序列分配内存块&quot;&quot;&quot;</span></span><br><span class="line">        blocks = <span class="variable language_">self</span>._allocate_blocks_for_token_ids(</span><br><span class="line">            prev_block=<span class="literal">None</span>, token_ids=token_ids, device=device)</span><br><span class="line">        <span class="variable language_">self</span>.update(blocks)</span><br><span class="line">        <span class="variable language_">self</span>._num_full_slots = <span class="built_in">len</span>(token_ids)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">append_token_ids</span>(<span class="params">self, token_ids: <span class="type">List</span>[<span class="built_in">int</span>], </span></span><br><span class="line"><span class="params">                         num_lookahead_slots: <span class="built_in">int</span> = <span class="number">0</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;追加新的 token 到块表&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 动态扩展块表以容纳新 token</span></span><br></pre></td></tr></table></figure>

<h3 id="4-4-前缀缓存优化"><a href="#4-4-前缀缓存优化" class="headerlink" title="4.4 前缀缓存优化"></a>4.4 前缀缓存优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PrefixCachingBlockAllocator</span>(<span class="title class_ inherited__">BlockAllocator</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现前缀缓存的块分配器&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">allocate_immutable_block</span>(<span class="params">self, prev_block: <span class="type">Optional</span>[Block], </span></span><br><span class="line"><span class="params">                                 token_ids: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; Block:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;分配不可变块，重用缓存块（如果可能）&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 1. 计算内容哈希</span></span><br><span class="line">        block = <span class="variable language_">self</span>._block_pool.init_block(prev_block, token_ids, ...)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 查找缓存块</span></span><br><span class="line">        cached_block_id = <span class="variable language_">self</span>._cached_blocks.get(block.content_hash, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> cached_block_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 缓存命中：重用已有块</span></span><br><span class="line">            block.block_id = cached_block_id</span><br><span class="line">            <span class="variable language_">self</span>._incr_refcount_cached_block(block)</span><br><span class="line">            <span class="keyword">return</span> block</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 缓存未命中：分配新块</span></span><br><span class="line">        block = <span class="variable language_">self</span>.allocate_mutable_block(prev_block)</span><br><span class="line">        block.append_token_ids(token_ids)</span><br><span class="line">        <span class="keyword">return</span> block</span><br></pre></td></tr></table></figure>

<h2 id="5-底层-CUDA-内核接口"><a href="#5-底层-CUDA-内核接口" class="headerlink" title="5. 底层 CUDA 内核接口"></a>5. 底层 CUDA 内核接口</h2><h3 id="5-1-C-操作接口"><a href="#5-1-C-操作接口" class="headerlink" title="5.1 C++ 操作接口"></a>5.1 C++ 操作接口</h3><h4 id="Paged-Attention-V1"><a href="#Paged-Attention-V1" class="headerlink" title="Paged Attention V1"></a>Paged Attention V1</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">paged_attention_v1</span>(<span class="params"></span></span><br><span class="line"><span class="params">    out: torch.Tensor,              <span class="comment"># 输出 [num_seqs, num_heads, head_size]</span></span></span><br><span class="line"><span class="params">    query: torch.Tensor,            <span class="comment"># 查询 [num_seqs, num_heads, head_size]  </span></span></span><br><span class="line"><span class="params">    key_cache: torch.Tensor,        <span class="comment"># Key 缓存</span></span></span><br><span class="line"><span class="params">    value_cache: torch.Tensor,      <span class="comment"># Value 缓存</span></span></span><br><span class="line"><span class="params">    num_kv_heads: <span class="built_in">int</span>,              <span class="comment"># KV 头数量</span></span></span><br><span class="line"><span class="params">    scale: <span class="built_in">float</span>,                   <span class="comment"># 注意力缩放因子</span></span></span><br><span class="line"><span class="params">    block_tables: torch.Tensor,     <span class="comment"># 块表 [num_seqs, max_blocks_per_seq]</span></span></span><br><span class="line"><span class="params">    seq_lens: torch.Tensor,         <span class="comment"># 序列长度 [num_seqs]</span></span></span><br><span class="line"><span class="params">    block_size: <span class="built_in">int</span>,                <span class="comment"># 块大小</span></span></span><br><span class="line"><span class="params">    max_seq_len: <span class="built_in">int</span>,               <span class="comment"># 最大序列长度</span></span></span><br><span class="line"><span class="params">    alibi_slopes: <span class="type">Optional</span>[torch.Tensor],  <span class="comment"># ALiBi 偏置斜率</span></span></span><br><span class="line"><span class="params">    kv_cache_dtype: <span class="built_in">str</span>,            <span class="comment"># KV 缓存数据类型</span></span></span><br><span class="line"><span class="params">    <span class="comment"># ... 其他参数</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure>

<h4 id="Paged-Attention-V2"><a href="#Paged-Attention-V2" class="headerlink" title="Paged Attention V2"></a>Paged Attention V2</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">paged_attention_v2</span>(<span class="params"></span></span><br><span class="line"><span class="params">    out: torch.Tensor,              <span class="comment"># 最终输出</span></span></span><br><span class="line"><span class="params">    exp_sum: torch.Tensor,          <span class="comment"># 指数和 [num_seqs, num_heads, max_partitions]</span></span></span><br><span class="line"><span class="params">    max_logits: torch.Tensor,       <span class="comment"># 最大 logits [num_seqs, num_heads, max_partitions]</span></span></span><br><span class="line"><span class="params">    tmp_out: torch.Tensor,          <span class="comment"># 临时输出 [num_seqs, num_heads, max_partitions, head_size]</span></span></span><br><span class="line"><span class="params">    <span class="comment"># ... 其他参数与 V1 相同</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br></pre></td></tr></table></figure>

<h3 id="5-2-KV-缓存操作"><a href="#5-2-KV-缓存操作" class="headerlink" title="5.2 KV 缓存操作"></a>5.2 KV 缓存操作</h3><h4 id="reshape-and-cache"><a href="#reshape-and-cache" class="headerlink" title="reshape_and_cache"></a>reshape_and_cache</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reshape_and_cache</span>(<span class="params"></span></span><br><span class="line"><span class="params">    key: torch.Tensor,              <span class="comment"># 输入 Key [num_tokens, num_heads, head_size]</span></span></span><br><span class="line"><span class="params">    value: torch.Tensor,            <span class="comment"># 输入 Value [num_tokens, num_heads, head_size]</span></span></span><br><span class="line"><span class="params">    key_cache: torch.Tensor,        <span class="comment"># Key 缓存</span></span></span><br><span class="line"><span class="params">    value_cache: torch.Tensor,      <span class="comment"># Value 缓存  </span></span></span><br><span class="line"><span class="params">    slot_mapping: torch.Tensor,     <span class="comment"># 插槽映射 [num_tokens]</span></span></span><br><span class="line"><span class="params">    kv_cache_dtype: <span class="built_in">str</span>,            <span class="comment"># 缓存数据类型</span></span></span><br><span class="line"><span class="params">    k_scale: torch.Tensor,          <span class="comment"># Key 量化缩放</span></span></span><br><span class="line"><span class="params">    v_scale: torch.Tensor,          <span class="comment"># Value 量化缩放</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将新的 KV 重塑并缓存到分页缓存中&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="6-平台特化实现"><a href="#6-平台特化实现" class="headerlink" title="6. 平台特化实现"></a>6. 平台特化实现</h2><h3 id="6-1-ROCm-AITER-优化"><a href="#6-1-ROCm-AITER-优化" class="headerlink" title="6.1 ROCm AITER 优化"></a>6.1 ROCm AITER 优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AITERPagedAttention</span>(<span class="title class_ inherited__">PagedAttention</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;ROCm 平台的 AITER 优化实现&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">write_to_paged_cache</span>(<span class="params">...</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用 ROCm AITER 的优化缓存写入&quot;&quot;&quot;</span></span><br><span class="line">        rocm_aiter.reshape_and_cache_with_pertoken_quant(...)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod  </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_decode</span>(<span class="params">...</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用块稀疏分页注意力进行优化&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> blocksparse_vert_stride &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 使用块稀疏 paged attention</span></span><br><span class="line">            <span class="keyword">return</span> PagedAttention.forward_decode(...)</span><br></pre></td></tr></table></figure>

<h3 id="6-2-HPU-平台实现"><a href="#6-2-HPU-平台实现" class="headerlink" title="6.2 HPU 平台实现"></a>6.2 HPU 平台实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HPUPagedAttentionMetadata</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;HPU 平台的 PagedAttention 元数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># HPU 特定的元数据结构</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HPUPagedAttention</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;HPU 平台的 PagedAttention 实现&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">write_to_paged_cache</span>(<span class="params">...</span>):</span><br><span class="line">        cache_ops.reshape_and_cache(...)  <span class="comment"># HPU 特化操作</span></span><br></pre></td></tr></table></figure>

<h2 id="7-性能优化特性"><a href="#7-性能优化特性" class="headerlink" title="7. 性能优化特性"></a>7. 性能优化特性</h2><h3 id="7-1-内存对齐优化"><a href="#7-1-内存对齐优化" class="headerlink" title="7.1 内存对齐优化"></a>7.1 内存对齐优化</h3><ul>
<li><strong>16字节对齐</strong>: Key 缓存使用 16 字节对齐以优化内存访问</li>
<li><strong>量化支持</strong>: 支持 INT8&#x2F;FP8 量化以减少内存占用</li>
<li><strong>零拷贝操作</strong>: 块交换使用零拷贝提升性能</li>
</ul>
<h3 id="7-2-块稀疏注意力"><a href="#7-2-块稀疏注意力" class="headerlink" title="7.2 块稀疏注意力"></a>7.2 块稀疏注意力</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 支持块稀疏模式以处理长序列</span></span><br><span class="line">blocksparse_local_blocks: <span class="built_in">int</span> = <span class="number">0</span>      <span class="comment"># 局部块数量</span></span><br><span class="line">blocksparse_vert_stride: <span class="built_in">int</span> = <span class="number">0</span>       <span class="comment"># 垂直步幅  </span></span><br><span class="line">blocksparse_block_size: <span class="built_in">int</span> = <span class="number">64</span>       <span class="comment"># 稀疏块大小</span></span><br><span class="line">blocksparse_head_sliding_step: <span class="built_in">int</span> = <span class="number">0</span> <span class="comment"># 头部滑动步长</span></span><br></pre></td></tr></table></figure>

<h3 id="7-3-分片计算优化"><a href="#7-3-分片计算优化" class="headerlink" title="7.3 分片计算优化"></a>7.3 分片计算优化</h3><ul>
<li><strong>分区大小</strong>: 默认 512 tokens&#x2F;partition，平衡内存与计算</li>
<li><strong>动态分区</strong>: 根据序列长度自动调整分区数量</li>
<li><strong>内存重用</strong>: V2 算法中的临时缓冲区可重用</li>
</ul>
<h2 id="8-前缀注意力支持"><a href="#8-前缀注意力支持" class="headerlink" title="8. 前缀注意力支持"></a>8. 前缀注意力支持</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward_prefix</span>(<span class="params">query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,</span></span><br><span class="line"><span class="params">                   kv_cache_dtype: <span class="built_in">str</span>, key_cache: torch.Tensor, </span></span><br><span class="line"><span class="params">                   value_cache: torch.Tensor, block_tables: torch.Tensor,</span></span><br><span class="line"><span class="params">                   query_start_loc: torch.Tensor, seq_lens_tensor: torch.Tensor,</span></span><br><span class="line"><span class="params">                   max_query_len: <span class="built_in">int</span>, alibi_slopes: <span class="type">Optional</span>[torch.Tensor],</span></span><br><span class="line"><span class="params">                   sliding_window: <span class="type">Optional</span>[<span class="built_in">int</span>]</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;前缀阶段的注意力计算&quot;&quot;&quot;</span></span><br><span class="line">    output = torch.empty_like(query)</span><br><span class="line">    context_attention_fwd(query, key, value, output, kv_cache_dtype,</span><br><span class="line">                          key_cache, value_cache, block_tables, </span><br><span class="line">                          query_start_loc, seq_lens_tensor, ...)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h2 id="9-块操作接口"><a href="#9-块操作接口" class="headerlink" title="9. 块操作接口"></a>9. 块操作接口</h2><h3 id="9-1-块交换"><a href="#9-1-块交换" class="headerlink" title="9.1 块交换"></a>9.1 块交换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">swap_blocks</span>(<span class="params">src_kv_cache: torch.Tensor, dst_kv_cache: torch.Tensor,</span></span><br><span class="line"><span class="params">                src_to_dst: torch.Tensor</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在 CPU/GPU 之间交换块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 分别处理 Key 和 Value 缓存</span></span><br><span class="line">    ops.swap_blocks(src_kv_cache[<span class="number">0</span>], dst_kv_cache[<span class="number">0</span>], src_to_dst)</span><br><span class="line">    ops.swap_blocks(src_kv_cache[<span class="number">1</span>], dst_kv_cache[<span class="number">1</span>], src_to_dst)</span><br></pre></td></tr></table></figure>

<h3 id="9-2-块复制"><a href="#9-2-块复制" class="headerlink" title="9.2 块复制"></a>9.2 块复制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">copy_blocks</span>(<span class="params">kv_caches: <span class="type">List</span>[torch.Tensor], </span></span><br><span class="line"><span class="params">                src_to_dists: torch.Tensor</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;复制块（用于 fork 操作）&quot;&quot;&quot;</span></span><br><span class="line">    key_caches = [kv_cache[<span class="number">0</span>] <span class="keyword">for</span> kv_cache <span class="keyword">in</span> kv_caches]</span><br><span class="line">    value_caches = [kv_cache[<span class="number">1</span>] <span class="keyword">for</span> kv_cache <span class="keyword">in</span> kv_caches]</span><br><span class="line">    ops.copy_blocks(key_caches, value_caches, src_to_dists)</span><br></pre></td></tr></table></figure>

<h2 id="10-关键技术创新"><a href="#10-关键技术创新" class="headerlink" title="10. 关键技术创新"></a>10. 关键技术创新</h2><h3 id="10-1-动态内存管理"><a href="#10-1-动态内存管理" class="headerlink" title="10.1 动态内存管理"></a>10.1 动态内存管理</h3><ul>
<li><strong>按需分配</strong>: 根据实际序列长度动态分配块</li>
<li><strong>内存池化</strong>: 使用 BlockPool 避免频繁的内存分配&#x2F;释放</li>
<li><strong>引用计数</strong>: 支持块的安全共享和释放</li>
</ul>
<h3 id="10-2-前缀缓存"><a href="#10-2-前缀缓存" class="headerlink" title="10.2 前缀缓存"></a>10.2 前缀缓存</h3><ul>
<li><strong>内容哈希</strong>: 基于内容计算哈希值实现前缀重用</li>
<li><strong>Copy-on-Write</strong>: 支持序列 fork 时的写时复制</li>
<li><strong>LRU 淘汰</strong>: 内存压力下的智能块淘汰策略</li>
</ul>
<h3 id="10-3-多设备支持"><a href="#10-3-多设备支持" class="headerlink" title="10.3 多设备支持"></a>10.3 多设备支持</h3><ul>
<li><strong>设备感知</strong>: 统一接口管理 CPU&#x2F;GPU 内存</li>
<li><strong>异步传输</strong>: 支持 CPU&#x2F;GPU 间的异步块传输</li>
<li><strong>平台优化</strong>: 针对不同硬件平台的专门优化</li>
</ul>
<h2 id="11-总结"><a href="#11-总结" class="headerlink" title="11. 总结"></a>11. 总结</h2><p>vLLM 的 Paged Attention 实现是一个高度优化的系统，通过以下关键技术实现了高效的内存管理：</p>
<ol>
<li><strong>分页内存管理</strong>: 将 KV 缓存组织成固定大小的页，实现灵活的内存分配</li>
<li><strong>版本化算法</strong>: V1&#x2F;V2 算法针对不同场景进行优化</li>
<li><strong>前缀缓存</strong>: 通过内容哈希实现计算结果的重用</li>
<li><strong>多平台支持</strong>: 针对不同硬件平台的特化实现</li>
<li><strong>块操作优化</strong>: 高效的块交换、复制和管理机制</li>
</ol>
<p>这个设计使得 vLLM 能够在保持高性能的同时，大幅提升 GPU 内存利用率，支持更大的批次大小和更长的序列处理。 </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/16/System/vLLM_PagedAttention_%E5%AE%9E%E7%8E%B0%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/" data-id="cmdpkiz60002cncon5wv89kgo" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/07/16/System/vLLM_safetensors%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE%E8%BD%AC%E6%8D%A2%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2025/07/16/System/PD%E5%88%86%E7%A6%BB%E6%9E%B6%E6%9E%84KV_Cache%E8%B7%A8%E8%8A%82%E7%82%B9%E4%BC%A0%E8%BE%93%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/System/">System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Test/">Test</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Communication/" rel="tag">Communication</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Compiler/" rel="tag">Compiler</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Graph/" rel="tag">Computational Graph</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepSeek/" rel="tag">DeepSeek</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distributed/" rel="tag">Distributed</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MoE/" rel="tag">MoE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization/" rel="tag">Optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Performance/" rel="tag">Performance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SGLang/" rel="tag">SGLang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/" rel="tag">System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/" rel="tag">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blues/" rel="tag">blues</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/career/" rel="tag">career</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/charts/" rel="tag">charts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/consensus/" rel="tag">consensus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/data-parallelism/" rel="tag">data-parallelism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/distributed-systems/" rel="tag">distributed-systems</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/experience/" rel="tag">experience</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/expert-parallelism/" rel="tag">expert-parallelism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/guitar/" rel="tag">guitar</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/harmony/" rel="tag">harmony</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/licks/" rel="tag">licks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/" rel="tag">life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mermaid/" rel="tag">mermaid</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/raft/" rel="tag">raft</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rhythm/" rel="tag">rhythm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/test/" rel="tag">test</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/Communication/" style="font-size: 10px;">Communication</a> <a href="/tags/Compiler/" style="font-size: 10px;">Compiler</a> <a href="/tags/Computational-Graph/" style="font-size: 10px;">Computational Graph</a> <a href="/tags/DeepSeek/" style="font-size: 10px;">DeepSeek</a> <a href="/tags/Distributed/" style="font-size: 10px;">Distributed</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/MoE/" style="font-size: 10px;">MoE</a> <a href="/tags/Optimization/" style="font-size: 10px;">Optimization</a> <a href="/tags/Performance/" style="font-size: 10px;">Performance</a> <a href="/tags/SGLang/" style="font-size: 10px;">SGLang</a> <a href="/tags/System/" style="font-size: 15px;">System</a> <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/blues/" style="font-size: 20px;">blues</a> <a href="/tags/career/" style="font-size: 10px;">career</a> <a href="/tags/charts/" style="font-size: 10px;">charts</a> <a href="/tags/consensus/" style="font-size: 10px;">consensus</a> <a href="/tags/data-parallelism/" style="font-size: 10px;">data-parallelism</a> <a href="/tags/distributed-systems/" style="font-size: 20px;">distributed-systems</a> <a href="/tags/experience/" style="font-size: 10px;">experience</a> <a href="/tags/expert-parallelism/" style="font-size: 10px;">expert-parallelism</a> <a href="/tags/guitar/" style="font-size: 20px;">guitar</a> <a href="/tags/harmony/" style="font-size: 10px;">harmony</a> <a href="/tags/licks/" style="font-size: 10px;">licks</a> <a href="/tags/life/" style="font-size: 10px;">life</a> <a href="/tags/llm/" style="font-size: 15px;">llm</a> <a href="/tags/mermaid/" style="font-size: 10px;">mermaid</a> <a href="/tags/raft/" style="font-size: 10px;">raft</a> <a href="/tags/rhythm/" style="font-size: 10px;">rhythm</a> <a href="/tags/test/" style="font-size: 10px;">test</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/05/">May 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/07/30/System/%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/%E5%BA%8F%E5%88%97%E5%B9%B6%E8%A1%8C/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/29/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/28/PD%E5%88%86%E7%A6%BB%E6%9E%B6%E6%9E%84KVCache%E8%B7%A8%E8%8A%82%E7%82%B9%E4%BC%A0%E8%BE%93%E5%8A%9F%E8%83%BD%E5%BC%80%E5%8F%91%EF%BC%8C%E4%BC%98%E5%8C%96KVCache%E4%BC%A0%E8%BE%93%E6%95%88%E7%8E%87/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/28/System/%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/EP/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/07/28/System/%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E5%9F%BA%E7%A1%80/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






<!-- Mermaid Support -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
<script>
  mermaid.initialize({
    startOnLoad: true,
    theme: 'default',
    securityLevel: 'loose',
    themeVariables: {
      primaryColor: '#0f4c75',
      primaryTextColor: '#fff',
      primaryBorderColor: '#0f4c75',
      lineColor: '#0f4c75',
      secondaryColor: '#006ba6',
      tertiaryColor: '#fff'
    }
  });
</script>
  </div>
</body>
</html>